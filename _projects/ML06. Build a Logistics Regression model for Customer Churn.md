---
layout: page
title: ML06. Build a Logistics Regression model for Customer Churn
description: another project with an image üéâ
img: assets/img/6.jpg
importance: 1
category: work
---

## Business Objective

Predicting a qualitative response for observation can be referred to as classifying that observation  since  it  involves  assigning  the  observation  to  a  category  or  class. Classification  forms  the  basis  for  Logistic  Regression.  Logistic  Regression  is  a  supervised  algorithm  used  to  predict  a  dependent  variable  that  is  categorical  or discrete. Logistic regression models the data using the sigmoid function.

Churned Customers  are those who have decided to end their relationship with their existing company. In our case study, we will be working on a churn dataset. XYZ  is  a  service-providing  company  that provides  customers  with  a  one-year subscription plan for their product. The company wants to know if the customers will renew the subscription for the coming year or not.

---

## Data Description

The CSV consists of around 2000 rows and 16 columns Features:

1. Year

2. Customer_id - unique id

3. Phone_no - customer phone no

4. Gender -Male/Female

5. Age

6. No of days subscribed - the number of days since the subscription

7. Multi-screen - does the customer have a single/ multiple screen subscription

8. Mail subscription - customer receive mails or not

9. Weekly mins watched - number of minutes watched weekly

10. Minimum daily mins - minimum minutes watched

11. Maximum daily mins - maximum minutes watched

12. Weekly nights max mins - number of minutes watched at night time

13. Videos watched - total number of videos watched

14. Maximum_days_inactive - days since inactive

15. Customer support calls - number of customer support calls  

16. Churn
    ‚óè 1- Yes
    ‚óè 0 - No

---

## Aim

Build a logistics regression learning model on the given dataset to determine whether
the customer will churn or not

---

## Approach  

1. Importing the required libraries and reading the dataset.

2. Inspecting and cleaning up the data

3. Perform data encoding on categorical variables

4. Exploratory Data Analysis (EDA)  
     - Data Visualization

5. Feature Engineering  
     - Dropping of unwanted columns

6. Model Building
     - Using the statsmodel library

7. Model Building
     - Performing train test split
     - Logistic Regression Model

8. Model Validation (predictions)
    - Accuracy score
    - Confusion matrix  
    - ROC and AUC
    - Recall score
    - Precision score
    - F1-score

9. Handling the unbalanced data  
     - With balanced weights
     - Random weights
     - Adjusting imbalanced data
     - Using SMOTE

10. Feature Selection
     - Barrier threshold selection
     - RFE method

11. Save the model in the form of a pickle file.

---

## Classification and Types of Problems

### Regression Problems

- Predict variable Y based on one or more X variables

- Uses continuous numerical values

- Fits a line through data points to predict expected Y values

### Classification Problems

- Y variable belongs to discrete classes rather than continuous values

- Types of classification:
  - Binary classification (Y = 0 or 1)
  - Multi-class classification (Y = 1, 2, 3, etc.)

- X variables can still be continuous

### Key Differences

- Regression predicts continuous values

- Classification distinguishes between discrete classes

- Models aim to separate classes rather than predict decimal values

Note: Classification models focus on determining which class a data point belongs to, rather than predicting exact numerical values.

---

### Real-World Classification Examples

- Spam Email Detection
  - Binary classification to identify spam vs legitimate emails

  - Uses features like email content, sender info, and patterns

- Customer Churn Prediction
  - Predicts which customers are likely to leave a service
  - Based on usage patterns, purchase history, and customer behavior

- Iris Flower Classification
  - Multi-class classification of flower species

  - Uses features like petal length, width, and other characteristics

---

## Mathematical Foundation of Logistic Regression

### Basic Equation

The fundamental equation for logistic regression is:

$$P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}} $$

### Logic Transformation

- Transform probability to log odds (logic):

- Start with base equation:

   $$ P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}} $$

- Transform to odds ratio:

   $$ \frac{P(y=1|x)}{1-P(y=1|x)} = e^{\beta_0 + \beta_1x} $$

- Take natural log of both sides:

   $$ ln(\frac{P(y=1|x)}{1-P(y=1|x)}) = \beta_0 + \beta_1x $$

### Key Properties

- The log odds (logit) is linear in x.

- Output probabilities are bounded between 0 and 1.

- The relationship between x and P(y=1|x) is S-shaped (sigmoid).

### Example: Customer Churn

In the customer churn example:

- y = 1: Customer churns
- y = 0: Customer stays
- x: Number of days customer was offline
- P(y=1|x): Probability of churning given offline days

This mathematical framework allows us to predict the probability of churn based on customer behavior patterns.
These examples demonstrate how classification can be applied to solve real business and scientific problems.

---

## From Linear to Logistic Regression: Key Changes

### 1. Log Odds Transformation

- Linear regression line is transformed into log odds function

- Points where y=1 map to positive infinity

- Points where y=0 map to negative infinity

### 2. Advantages of Log Odds

- Parameters can search full range (-‚àû to +‚àû)

- Not restricted to [0,1] interval like probabilities

### 3. Changes in Model Fitting

- Cannot use traditional sum of squared residuals

  - Residuals would be infinite due to mapping to ¬±‚àû

  - Solution: Use maximum likelihood estimation instead

### 4. Coefficient Interpretation

- Coefficients represent relationship with log odds

- Less intuitive for business insights

- Requires careful interpretation for practical applications

Note: Understanding these transformations is crucial for proper implementation and interpretation of logistic regression models.

---

## Data Processing

In this technical implementation walkthrough, we begin by accessing a Demo Notebook and managing computational resources efficiently by selectively running commands based on their processing requirements. The data loading process involves accessing a dataset stored on Google Drive using Google Cloud libraries to import it into a pandas DataFrame. This implementation requires a one-time authentication step through the mount command, which establishes secure access to the Google account. Once authenticated, the process of loading the file into a pandas DataFrame becomes streamlined and efficient. This approach demonstrates a practical method for data acquisition and preparation in a cloud-based environment, setting the foundation for subsequent data analysis tasks.

---

```python
# import pandas
import pandas as pd
df=pd.read_csv("data_regression.csv")

```

---

```python
# get the first 10 rows
df.head(10)

```

---


---

- The dataset presents a customer churn prediction problem where we aim to forecast whether a customer will churn in upcoming months based on their previous month's activity. The dataset includes various features such as customer signup year, customer ID, phone number, gender, age, subscription duration, multi-screen status, and male subscription status, which are categorical variables.
  
- Additionally, behavioral variables include weekly minutes watched, minimum daily minutes, maximum daily minutes, weekly maximum viewing minutes, number of videos watched, maximum inactive days, and customer support calls. The target variable indicates whether the customer has churned.

- To begin the data analysis process, a function was implemented to examine data types and handle missing values. The function returns the data frame types, counts missing values, and generates a heat map to visualize the distribution of missing data.

---

## **Inspecting and cleaning up the frame**

```python
# check for the missing values and dataframes
def inspection(dataframe):
  import pandas as pd
  import seaborn as sns
  print("Types of the variables we are working with:")
  print(dataframe.dtypes) # dtypes
  
  print("Total Samples with missing values:")

  print(df.isnull().any(axis=1).sum()) # null values

  print("Total Missing Values per Variable")
  print(df.isnull().sum())
  print("Map of missing values")
  sns.heatmap(dataframe.isnull())

```

---

```python
inspection(df)

```

---

<Table 

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/f4.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

- The analysis revealed 82 samples with missing values across different variables. Notably, there were 35 missing values in the churn column, along with missing data in the maximum days inactive and gender columns. The heat map visualization clearly displayed these missing value patterns through distinct lines.

- Given the relatively small amount of missing data and the critical importance of the churn variable, the decision was made to drop rows with missing values using a simple pandas command to create a clean dataset. The next steps in the analysis will involve data visualization and handling categorical variables, which will be crucial for building an effective model.

---

```python
df = df.dropna() # cleaning up null value

```

---

### **Encoding categorical variables**

- The code outlines the procedure for transforming categorical variables, initially stored as strings (e.g., "yes" or "no"), into numerical representations to facilitate their use in predictive modeling. Variables such as gender, multi-screen usage, and subscription status are identified as categorical, and a specific function is applied to encode these string values into numerical codes. This process generates new columns in the data frame, such as gender_code and multi_screen_code, which can then be incorporated into analytical models.

- The discussion also addresses the interpretation of coefficients in logistic regression, noting that these coefficients quantify the change in log odds for a one-unit increase in the predictor variable. For binary variables, this translates to the effect of being in one category (encoded as 1) compared to another (encoded as 0) on the odds of the target outcome. The interpretation remains consistent, and the application of this approach will be further illustrated in subsequent model evaluations.

---

```python
df.head(5)

df.multi_screen.unique()

```

> array(['no', 'yes'], dtype=object)

---

```python
from sklearn.preprocessing import OrdinalEncoder

def encode_categories(df, variables, suffix='_code', inplace=False):
    """
    Encodes categorical variables in a DataFrame using OrdinalEncoder.
    
    Parameters:
        df (pd.DataFrame): The input DataFrame.
        variables (list): List of categorical column names to encode.
        suffix (str): Suffix to append to encoded column names. Default is '_code'.
        inplace (bool): If True, modifies the DataFrame in place. If False, returns a new DataFrame.
    
    Returns:
        pd.DataFrame: DataFrame with encoded columns.
    """
    # Check if the input variables exist in the DataFrame
    for v in variables:
        if v not in df.columns:
            raise ValueError(f"Variable '{v}' not found in DataFrame.")
    
    # Create a copy of the DataFrame if not modifying in place
    if not inplace:
        df = df.copy()
    
    # Initialize the OrdinalEncoder once (efficient for multiple variables)
    ord_enc = OrdinalEncoder()
    
    # Encode each categorical variable
    for v in variables:
        # Create a new column name with the specified suffix
        name = v + suffix
        
        # Fit and transform the categorical variable into numerical codes
        df[name] = ord_enc.fit_transform(df[[v]])
        
        # Print unique encoded values for verification
        print(f'The encoded values for {v} are: {df[name].unique()}')
    
    # Return the DataFrame with encoded columns
    return df

```

---

### Explaination

- The provided code defines a function, encode_categories, which encodes categorical variables in a DataFrame using OrdinalEncoder from sklearn.preprocessing. Here's a brief explanation:

**Function Purpose**:

- The function takes a DataFrame (df) and a list of categorical variables (variables) as inputs and encodes these variables into numerical values.

**Encoding Process**:

- It initializes an OrdinalEncoder object.

- For each variable in the list, it creates a new column in the DataFrame with a suffix (_code by default) to store the encoded values.

- The encoder transforms the categorical values into numerical codes using fit_transform.

**Output**:

- The function prints the unique encoded values for each variable.

- It modifies the DataFrame in place by default but can return a new DataFrame if inplace=False.

**Key Features:**

- Avoids redundant encoder initialization.

- Includes input validation to check if variables exist in the DataFrame.

- Allows customization of the suffix for encoded columns.

---

```python
df 

```

---

Table
---

```python
# check for the encoded variables
encode_categories (df,['gender','multi_screen','mail_subscribed'])

```

> The encoded values for gender are: [0. 1.]
> The encoded values for multi_screen are: [0. 1.]
> The encoded values for mail_subscribed are: [0. 1.]

---

TAble

---

## **Do some data visualizations**

```python
def plot_scatterplots (df,cols_to_exclude,class_col):
  #this function returns scatterplots of all the variables in the dataset
  #against the classification variable,
  #for a quick data visualization
  import numpy as np
  import seaborn as sns
  import warnings
  warnings.filterwarnings("ignore")
  cols=df.select_dtypes(include=np.number).columns.tolist() #finding all the numerical columns from the dataframe
  X=df[cols] #creating a dataframe only with the numerical columns
  X = X[X.columns.difference(cols_to_exclude)] #columns to exclude
  for col in X.columns.difference([class_col]):
    g = sns.FacetGrid(df)
    g.map(sns.scatterplot, col, class_col)

```

---

```python
# plot 
plot_scatterplots (df,['customer_id','phone_no', 'year'],'churn')

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k1.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k2.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k3.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k4.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k3.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k6.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k7.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k8.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k9.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k10.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k11.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k12.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
def full_diagnostic(df,class_col,cols_to_exclude):
  import seaborn as sns
  import numpy as np
  cols=df.select_dtypes(include=np.number).columns.tolist() #finding all the numerical columns from the dataframe
  X=df[cols] #creating a dataframe only with the numerical columns
  X = X[X.columns.difference(cols_to_exclude)] #columns to exclude
  X = X[X.columns.difference([class_col])]
  sns.pairplot(df,hue = class_col)

```

---

```python
full_diagnostic(df,class_col = 'churn',cols_to_exclude=['customer_id','phone_no', 'year'])

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k14.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
# function for creating plots for selective columns only
def selected_diagnotic(df,class_col,cols_to_eval):
  import seaborn as sns
  cols_to_eval.append(class_col) 
  X = df[cols_to_eval] # only selective columns
  sns.pairplot(X,hue = class_col) # plot

```

---

```python
selected_diagnotic(df,class_col = 'churn',cols_to_eval = ['videos_watched','no_of_days_subscribed'])

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k17.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

## **Run the logistic regression**

```python
def logistic_regression(df,class_col,cols_to_exclude):
  import statsmodels.api as sm
  import numpy as np
  cols=df.select_dtypes(include=np.number).columns.tolist() 
  X=df[cols]
  X = X[X.columns.difference([class_col])] 
  X = X[X.columns.difference(cols_to_exclude)] # unwanted columns 

  ## Scaling variables
  ##from sklearn import preprocessing
  ##scaler = preprocessing.StandardScaler().fit(X)
  ##X_scaled = scaler.transform(X)


  #X_Scale = scaler.transform(X)
  y=df[class_col] # the target variable 
  logit_model=sm.Logit(y,X) 
  result=logit_model.fit() # fit the model 
  print(result.summary2()) # check for summary 

```

---

```python
logistic_regression(df,class_col = 'churn',cols_to_exclude=['customer_id','phone_no', 'year'])

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k18.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
import math
math.exp(1.9511)

```

> 7.036423390843554

---

## **Run the ML Model**

```python
def prepare_model(df,class_col,cols_to_exclude):
## Split in training and test set
  from sklearn.model_selection import train_test_split
  import numpy as np
  ##Selecting only the numerical columns and excluding the columns we specified in the function
  cols=df.select_dtypes(include=np.number).columns.tolist() 
  X=df[cols]
  X = X[X.columns.difference([class_col])] 
  X = X[X.columns.difference(cols_to_exclude)]
  ##Selecting y as a column
  y=df[class_col]
  global X_train, X_test, y_train, y_test #This allow us to do call these variables outside this function
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # perform train test split

```

---

```python
def run_model(X_train,X_test,y_train,y_test):
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import roc_auc_score,classification_report
  global logreg #Defines the logistic model as a global model that can be used outside of this function
  ##Fitting the logistic regression
  logreg = LogisticRegression(random_state = 13)
  logreg.fit(X_train, y_train) # fit the model
  ##Predicting y values
  global y_pred #Defines the Y_Pred as a global variable that can be used outside of this function
  y_pred = logreg.predict(X_test) # make predictions on th test data
  logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))
  print(classification_report(y_test, y_pred)) # check for classification report 
  print("The area under the curve is: %0.2f"%logit_roc_auc)  # check for  AUC
     

```

---

```python
prepare_model(df,class_col='churn',cols_to_exclude=['customer_id','phone_no', 'year'])

```

---

```python
run_model(X_train,X_test,y_train,y_test)

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k19.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
def confusion_matrix(y_test,y_pred):
  from sklearn.metrics import confusion_matrix
  confusion_matrix = confusion_matrix(y_test, y_pred) # confusion matrix 
  print(confusion_matrix)

  tn, fp, fn, tp = confusion_matrix.ravel()
  print('TN: %0.2f'% tn)
  print('TP: %0.2f'% tp)
  print('FP: %0.2f'%fp)
  print('FN: %0.2f'%fn)

```

---

```python
def roc_curve (logreg,X_test,y_test):
  import matplotlib.pyplot as plt 
  from sklearn.metrics import roc_auc_score
  from sklearn.metrics import roc_curve
  logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test)) # ROC AUC score 
  fpr, tpr, thresholds = roc_curve(y_test, logreg.predict(X_test)) # ROC curve
  #Setting the graph area
  plt.figure()
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])

  #Plotting the worst line possiple
  plt.plot([0, 1], [0, 1],'b--')

  #Plotting the logistic regression we have built
  plt.plot(fpr, tpr, color='darkorange', label='Logistic Regression (area = %0.2f)' % logit_roc_auc)

  #Adding labels and etc
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('ROC Curve')
  plt.legend(loc="lower right")
  plt.savefig('Log_ROC')
  plt.show()

```

---

```python
prepare_model(df,class_col='churn',cols_to_exclude=['customer_id','phone_no', 'year'])

```

---


```python
confusion_matrix(y_test,y_pred)

```

[[504   9]
 [ 55   8]]
TN: 504.00
TP: 8.00
FP: 9.00
FN: 55.00

---

```python
roc_curve (logreg,X_test,y_test)

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k20.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

## **Saving & Running the Model**

```python
# save the model using pickle function 
import pickle
pickle.dump(logreg, open('model1.pkl', 'wb'))

```

---

```python
# load the saved model 
model = pickle.load(open('model1.pkl', 'rb'))

```

---

```python
# make predictions on the test data
model.predict(X_test)

```

array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

---

## **Dealing with Class Imbalance**

```python
# check the target variable
df['churn'].describe()

```

count    1918.000000
mean        0.131908
std         0.338479
min         0.000000
25%         0.000000
50%         0.000000
75%         0.000000
max         1.000000
Name: churn, dtype: float64

---

```python
# class imbalance method 1 
def run_model_bweights(X_train,X_test,y_train,y_test):
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import roc_auc_score,classification_report
    global logreg
    logreg = LogisticRegression(random_state = 13,class_weight = 'balanced') # define class_weight parameter
    logreg.fit(X_train, y_train) # fit the model 
    global y_pred
    y_pred = logreg.predict(X_test) # predict on test data
    logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test)) # ROC AUC score
    print(classification_report(y_test, y_pred)) 
    print("The area under the curve is: %0.2f"%logit_roc_auc) # AUC curve

```

---

```python
run_model_bweights(X_train,X_test,y_train,y_test)

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k20.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
# class imbalance method 2
def run_model_aweights(X_train,X_test,y_train,y_test,w):
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import roc_auc_score,classification_report
    global logreg
    logreg = LogisticRegression(random_state = 13,class_weight=w) # define class_weight parameter
    logreg.fit(X_train, y_train) # fit the model 
    global y_pred
    y_pred = logreg.predict(X_test) # predict on test data
    logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))  # ROC AUC score
    print(classification_report(y_test, y_pred))
    print("The area under the curve is: %0.2f"%logit_roc_auc)  # AUC curve

```

---

```python
run_model_aweights(X_train,X_test,y_train,y_test,{0:90, 1:10})

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k21.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
# class imbalance method 3
def adjust_imbalance (X_train,y_train,class_col):
  from sklearn.utils import resample
  import pandas as pd
  X = pd.concat([X_train, y_train], axis=1)

  # separate the 2 classes
  class0 = X[X[class_col]==0]
  class1 = X[X[class_col]==1]

  # Case 1 - bootstraps from the minority class
  if len(class1)<len(class0):
    resampled = resample(class1,
                              replace=True, 
                              n_samples=len(class0), 
                              random_state=10) 
    resampled_df = pd.concat([resampled, class0])

  # Case 1 - ressamples from the majority class
  else:
    resampled = resample(class1,
                              replace=False, 
                              n_samples=len(class0), 
                              random_state=10) 
    resampled_df = pd.concat([resampled, class0])

  return resampled_df

```

---

```python
resampled_df = adjust_imbalance (X_train,y_train,class_col = 'churn')

```

---

```python
prepare_model(resampled_df,class_col = 'churn',cols_to_exclude=['customer_id','phone_no', 'year'])
run_model(X_train,X_test,y_train,y_test)

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k22.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
def prepare_model_smote(df,class_col,cols_to_exclude):
#Synthetic Minority Oversampling Technique. Generates new instances from existing minority cases that you supply as input. 
  from sklearn.model_selection import train_test_split
  import numpy as np
  from imblearn.over_sampling import SMOTE
  cols=df.select_dtypes(include=np.number).columns.tolist() 
  X=df[cols]
  X = X[X.columns.difference([class_col])]
  X = X[X.columns.difference(cols_to_exclude)]
  y=df[class_col]
  global X_train, X_test, y_train, y_test
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
  sm = SMOTE(random_state=0, sampling_strategy=1.0)
  X_train, y_train = sm.fit_resample(X_train, y_train) 

```

---

```python
prepare_model_smote(df,class_col = 'churn',cols_to_exclude=['customer_id','phone_no', 'year'])
run_model(X_train,X_test,y_train,y_test)

```

---

## **Predictions**

```python
run_model(X_train,X_test,y_train,y_test)

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k24.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

## **Feature selection**

```python
class_col = 'churn'
cols_to_exclude=['customer_id','phone_no', 'year']

# function for feature selection 
def var_threshold_selection(df,cols_to_exclude,class_col,threshold):
  from sklearn.feature_selection import VarianceThreshold
  import numpy as np
  from sklearn import preprocessing

  cols=df.select_dtypes(include=np.number).columns.tolist() #finding all the numerical columns from the dataframe
  X=df[cols] #creating a dataframe only with the numerical columns
  X = X[X.columns.difference(cols_to_exclude)] #columns to exclude
  X = X[X.columns.difference([class_col])]
  ## Scaling variables
  scaler = preprocessing.StandardScaler().fit(X)
  X_scaled = scaler.transform(X)
  var_thr = VarianceThreshold(threshold = threshold) #Removing both constant and quasi-constant
  var_thr.fit(X_scaled)
  var_thr.get_support()

  global selected_cols
  selected_cols = X.columns[var_thr.get_support()]

  print("The selected features are: ")
  print(list(selected_cols))

```

---

```python
var_threshold_selection(df,cols_to_exclude=['customer_id','phone_no', 'year'],class_col = 'churn',threshold=1)

```

> The selected features are: 
> ['maximum_daily_mins', 'maximum_days_inactive', 'weekly_mins_watched']

---

```python
prepare_model(resampled_df,class_col = 'churn',cols_to_exclude=['customer_id','phone_no', 'year',
                                                                'gender', 'age',
                                                                'no_of_days_subscribed', 'multi_screen', 'mail_subscribed', 'minimum_daily_mins', 
                                                                'weekly_max_night_mins', 'videos_watched', 
                                                                'customer_support_calls', 'churn', 'gender_code', 'multi_screen_code',
                                                                'mail_subscribed_code'])
run_model(X_train,X_test,y_train,y_test)

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k25.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
# RFE for feature selection 
def rfe_selection(df,cols_to_exclude,class_col,model):
  import warnings
  warnings.filterwarnings("ignore")
  import numpy as np
  from sklearn.feature_selection import RFE

  cols=df.select_dtypes(include=np.number).columns.tolist() #finding all the numerical columns from the dataframe
  X=df[cols] #creating a dataframe only with the numerical columns
  X = X[X.columns.difference(cols_to_exclude)] #columns to exclude
  X = X[X.columns.difference([class_col])]
  y = df[class_col]

  rfe = RFE(model)
  rfe = rfe.fit(X, y) # fit the model 
  global selected_cols
  selected_cols = X.columns[rfe.support_]

  print("The selected features are: ")
  print(list(selected_cols))

```

---

```python
# RFE for feature selection 
def rfe_selection(df,cols_to_exclude,class_col,model):
  import warnings
  warnings.filterwarnings("ignore")
  import numpy as np
  from sklearn.feature_selection import RFE

  cols=df.select_dtypes(include=np.number).columns.tolist() #finding all the numerical columns from the dataframe
  X=df[cols] #creating a dataframe only with the numerical columns
  X = X[X.columns.difference(cols_to_exclude)] #columns to exclude
  X = X[X.columns.difference([class_col])]
  y = df[class_col]

  rfe = RFE(model)
  rfe = rfe.fit(X, y) # fit the model 
  global selected_cols
  selected_cols = X.columns[rfe.support_]

  print("The selected features are: ")
  print(list(selected_cols))

```

> The selected features are: 
> ['customer_support_calls', 'gender_code', 'mail_subscribed_code', 'maximum_days_inactive', 'minimum_daily_mins', 'multi_screen_code']

---

```python
prepare_model(resampled_df,class_col = 'churn',cols_to_exclude=['customer_id','phone_no', 'year',
                                                                'gender', 'age',
                                                                'no_of_days_subscribed', 'multi_screen', 'mail_subscribed', 
                                                                'weekly_max_night_mins', 
                                                                 'gender_code', 'multi_screen_code',
                                                                'mail_subscribed_code'])
run_model(X_train,X_test,y_train,y_test)

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/k26.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---