---
layout: page
title: ML04. Learn to Build a Polynomial Regression Model from Scratch 
description: another without an image
img: /assets/img/basketball.png
importance: 1
category: work
---

1.0 Configure your Azure ML workspace
---

## **1.1 Workspace parameters**

- To use an AML Workspace, you will need to import the Azure ML SDK and supply the following information:
  - Your subscription id

  - A resource group name

  - (optional) The region that will host your workspace

  - A name for your workspace

- You can get your subscription ID from the [Azure portal](https://portal.azure.com).

- You will also need access to a [_resource group_](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview#resource-groups), which organizes Azure resources and provides a default region for the resources in a group.  You can see what resource groups to which you have access, or create a new one in the [Azure portal](https://portal.azure.com).  If you don't have a resource group, the create workspace command will create one for you using the name you provide.

- The region to host your workspace will be used if you are creating a new workspace.  You do not need to specify this if you are using an existing workspace. You can find the list of supported regions [here](https://azure.microsoft.com/en-us/global-infrastructure/services/?products=machine-learning-service).  You should pick a region that is close to your location or that contains your data.

- The name for your workspace is unique within the subscription and should be descriptive enough to discern among other AML Workspaces.  The subscription may be used only by you, or it may be used by your department or your entire enterprise, so choose a name that makes sense for your situation.

- The following cell allows you to specify your workspace parameters.  This cell uses the python method `os.getenv` to read values from environment variables which is useful for automation.  If no environment variable exists, the parameters will be set to the specified default values.  

- If you ran the Azure Machine Learning [quickstart](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started) in Azure Notebooks, you already have a configured workspace!  You can go to your Azure Machine Learning Getting Started library, view *config.json* file, and copy-paste the values for subscription ID, resource group and workspace name below.

Replace the default values in the cell below with your workspace parameters

---

## **1.2 Access your workspace**

- The following cell uses the Azure ML SDK to attempt to load the workspace specified by your parameters.  If this cell succeeds, your notebook library will be configured to access the workspace from all notebooks using the `Workspace.from_config()` method.  The cell can fail if the specified workspace doesn't exist or you don't have permissions to access it.

---

```python
from azureml.core import Workspace

try:
    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)
    # write the details of the workspace to a configuration file to the notebook library
    ws.write_config()
    print("Workspace configuration succeeded. Skip the workspace creation steps below")
except:
    print("Workspace not accessible. Change your parameters or create a new workspace below")
```

---

### **1.2.1 Create a new workspace**

- If you don't have an existing workspace and are the owner of the subscription or resource group, you can create a new workspace.  If you don't have a resource group, the create workspace command will create one for you using the name you provide.

**Note**: As with other Azure services, there are limits on certain resources (for example AmlCompute quota) associated with the Azure ML service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.

- This cell will create an Azure ML workspace for you in a subscription provided you have the correct permissions.

This will fail if:
* You do not have permission to create a workspace in the resource group

* You do not have permission to create a resource group if it's non-existing.

* You are not a subscription owner or contributor and no Azure ML workspaces have ever been created in this subscription

If workspace creation fails, please work with your IT admin to provide you with the appropriate permissions or to provision the required resources.

**Note**: A Basic workspace is created by default. If you would like to create an Enterprise workspace, please specify sku = 'enterprise'.
Please visit our [pricing page](https://azure.microsoft.com/en-us/pricing/details/machine-learning/) for more details on our Enterprise edition.

---

```python
from azureml.core import Workspace

# Create the workspace using the specified parameters
ws = Workspace.create(name = "WS004_Regression_Splines",
                    subscription_id = "88187483-a72c-4f6a-83e5-43f1855302e1",
                    resource_group = "RG004-Regression_Splines", 
                    location = "South Africa North",
                    create_resource_group = True,
                    sku = 'basic',
                    exist_ok = True)
ws.get_details()

# write the details of the workspace to a configuration file to the notebook library
ws.write_config()
```

---

### **1.2.2 Create compute resources for your training experiments**

- Many of the sample notebooks use Azure ML managed compute (AmlCompute) to train models using a dynamically scalable pool of compute. In this section you will create default compute clusters for use by the other notebooks and any other operations you choose.

> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.

- To create a cluster, you need to specify a compute configuration that specifies the type of machine to be used and the scalability behaviors.  Then you choose a name for the cluster that is unique within the workspace that can be used to address the cluster later.

The cluster parameters are:

- vm_size - this describes the virtual machine type and size used in the cluster.  All machines in the cluster are the same type.  You can get the list of vm sizes available in your region by using the CLI command

```shell
az vm list-skus -o tsv
```

- min_nodes - this sets the minimum size of the cluster.  If you set the minimum to 0 the cluster will shut down all nodes while not in use.  Setting this number to a value higher than 0 will allow for faster start-up times, but you will also be billed when the cluster is not in use.

- max_nodes - this sets the maximum size of the cluster.  Setting this to a larger number allows for more concurrency and a greater distributed processing of scale-out jobs.

- To create a **CPU** cluster now, run the cell below. The autoscale settings mean that the cluster will scale down to 0 nodes when inactive and up to 4 nodes when busy.

---

```python
from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.core.compute_target import ComputeTargetException

# Choose a name for your CPU cluster
cpu_cluster_name = "cpu-cluster"

# Verify that cluster does not exist already
try:
    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)
    print("Found existing cpu-cluster")
except ComputeTargetException:
    print("Creating new cpu-cluster")
    
    # Specify the configuration for the new cluster
    compute_config = AmlCompute.provisioning_configuration(vm_size="STANDARD_D2_V2",
                                                        min_nodes=0,
                                                        max_nodes=4)

    # Create the cluster with the specified name and configuration
    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)
    
    # Wait for the cluster to complete, show the output log
    cpu_cluster.wait_for_completion(show_output=True)
```

---
## **Project Overview**

The National Basketball Association (NBA) is one of the most popular professional sports leagues in the world, with a large and dedicated fan base. Understanding the factors that contribute to a team's ability to score points is crucial for coaches, team managers, and owners, as it can inform decisions related to player acquisition, training and nutrition, and game strategy.

Knowing the points scored by a particular player beforehand can also help the stakeholders to make informed decisions on the playing time and role of the player in the team. For example, if a team has a player who is consistently scoring a high number of points, it might make sense to give them more playing time or to make them a focal point of the team's offensive strategy.

Additionally, predicting the points scored by a team can also help the stakeholders to make better decisions on game day. For example, if a team is expected to score a high number of points in a game, it might make sense to adopt a more aggressive offensive strategy or to focus on defending against the opposing team's best scorers. On the other hand, if a team is not expected to score many points, it might make sense to adopt a more defensive strategy or to focus on controlling the pace of the game.

In short, being able to accurately predict the points scored by a team can be a powerful tool for coaches, team managers, and owners as it can inform decisions related to player acquisition, training and nutrition, and game strategy, as well as help them to make better decisions on game day. Ultimately, this can lead to a more successful team, which can result in increased revenue and fan engagement.

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/basketball.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

In this project, we use a dataset of various attributes such as team name, points scored, average number of weightlifting sessions, average number of yoga sessions, average number of laps run per practice, water intake, and number of players absent for sessions from the NBA to predict the points scored by a sports team using polynomial regression.

With the business implications cleared. Let's get into the project's technical details.

---

## **Learning Outcomes**

- Understanding of how to perform Exploratory Data Analysis (EDA) on the dataset, including cleaning, transforming, and visualizing the data.

- Knowledge of different imputation techniques to handle missing data.

- Understanding the theory and concepts behind polynomial regression, including the difference between linear and polynomial regression, and the relationship between the independent and dependent variables.

- Understanding of how to evaluate the goodness of fit of a regression model, including the use of R-squared, Mean Squared Error (MSE), and other performance metrics.

- Understanding of the Anova table, residual plots, and how to perform model diagnostics to identify and address model assumptions and limitations.

- Knowledge of the Chatterjee Correlation and how to use it to evaluate the relationship between the independent and dependent variables in a polynomial regression model.

- Understanding of AIC and BIC, likelihood, and the log likelihood, and how to use them to compare and select the best-fitting polynomial regression model.
To give your project a background in the portfolio page, just add the img tag to the front matter like so:

---

## **Approach**

- Data Preprocessing
  - Outlier removal

  - Imputing null values

  - Onehot encoding

- Model Building
  - Linear regression model building

  - Polynomial regression model building

- Model Evaluation

  - Valuation of model on test data

  - Discussion on various regression matrix-like R-squared, AIC, and F statistics.

## **Important Libraries**

- **Pandas**: pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool built on top of the Python programming language. Refer to [documentation](https://pandas.pydata.org/) for more information.

- **NumPy**: The fundamental package for scientific computing with Python. Fast and versatile, the NumPy vectorization, indexing, and broadcasting concepts are the de-facto standards of array computing today. NumPy offers comprehensive mathematical functions, random number generators, linear algebra routines, Fourier transforms, and more. Refer to [documentation](https://numpy.org/) for more information. pandas and NumPy are together used for most of the data analysis and manipulation in Python.

- **Matplotlib**: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Refer to [documentation](https://matplotlib.org/) for more information.

- **Seaborn**: Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Refer to [documentation](https://seaborn.pydata.org/) for more information.

- **Scikit-learn**: Simple and efficient tools for predictive data analysis
accessible to everybody and reusable in various contexts.
It is built on NumPy, SciPy, and matplotlib to support machine learning in Python. Refer to [documentation](https://scikit-learn.org/stable/) for more information.

-**Statsmodels**: statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests and statistical data exploration. Refer to [documentation](https://www.statsmodels.org/stable/index.html) for more information.

---

## **Install Packages**

```python
#importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import sys
from matplotlib.pyplot import figure
import matplotlib.pyplot as plt

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import statsmodels.api as sm
from sklearn.metrics import mean_absolute_error,mean_squared_error
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.cm as cm
import matplotlib.colors as mcolors
```

---

## **Install Packages**

```python
#reading data
data = pd.read_csv('Input/NBA_Dataset_csv.csv')
```

---

```python
#view top 3 entries
data.head()
```

---

<div style="display: flex; justify-content: center; width: 100%; overflow-x: auto;">
<div style="max-width: 100%;">
<style scoped>
    .dataframe {
        margin: 0 auto;  /* Centers the table */
        width: auto;     /* Allows table to size to content */
        max-width: 100%; /* Prevents table from overflowing container */
    }
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: center;
        white-space: nowrap; /* Prevents header text from wrapping */
    }
    .dataframe tbody td {
        white-space: nowrap; /* Prevents cell text from wrapping */
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Team</th>
      <th>Points_Scored</th>
      <th>Weightlifting_Sessions_Average</th>
      <th>Yoga_Sessions_Average</th>
      <th>Laps_Run_Per_Practice_Average</th>
      <th>Water_Intake</th>
      <th>Players_Absent_For_Sessions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Lakers</td>
      <td>242</td>
      <td>23.0</td>
      <td>25.0</td>
      <td>30.0</td>
      <td>5.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Lakers</td>
      <td>144</td>
      <td>24.0</td>
      <td>26.0</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Lakers</td>
      <td>156</td>
      <td>24.0</td>
      <td>27.0</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Lakers</td>
      <td>159</td>
      <td>26.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Lakers</td>
      <td>106</td>
      <td>27.0</td>
      <td>NaN</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>6.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

---

```python
# shape of the data
print("Dimension of the dataset is= ",data.shape)
```

> Dimension of the dataset is=  (159, 7)

---

```python
# shape of the data
print("Dimension of the dataset is= ",data.shape)
```

> Index(['Team', 'Points_Scored',       'Weightlifting_Sessions_Average','Yoga_Sessions_Average', 'Laps_Run_Per_Practice_Average', 'Water_Intake','Players_Absent_For_Sessions'],
> dtype='object')

---

## **Data Dictionary**

- **Team**: name of the team

- **Points_Scored**: points scored by the team

- **Weightlifting_Sessions_Average**: weighlifting sessions on an average done by the team

- **Yoga_Sessions_Average**: yoga sessions on an average done by the team

 **Laps_Run_Per_Practice_Average**: laps run on an average by the team

- **Water_Intake**: total water intake

- **Players_Absent_For_Sessions**: number of players absent for sessions

---

## **Exploratory Data Analysis**

Exploratory Data Analysis, commonly known as EDA, is a technique to analyze the data with visuals. It involves using statistics and visual techniques to identify particular trends in data.

It is used to understand data patterns, spot anomalies, check assumptions, etc. The main purpose of EDA is to help look into the data before making any hypothesis about it.

---

```python
# info of the data
data.info()
```

```python
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 159 entries, 0 to 158
Data columns (total 7 columns):
 #   Column                          Non-Null Count  Dtype  
---  ------                          --------------  -----  
 0   Team                            159 non-null    object 
 1   Points_Scored                   159 non-null    int64  
 2   Weightlifting_Sessions_Average  150 non-null    float64
 3   Yoga_Sessions_Average           139 non-null    float64
 4   Laps_Run_Per_Practice_Average   145 non-null    float64
 5   Water_Intake                    145 non-null    float64
 6   Players_Absent_For_Sessions     130 non-null    float64
dtypes: float64(5), int64(1), object(1)
memory usage: 8.8+ KB
```

---

There are total 159 rows and 7 columns in the NBA Dataset.

Observe that there are a few null values in the dataset.

Learn about Essential basic functionality for pandas dataframe [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes).

---

```python
# renaming required columns
df=data.rename(columns={'Points_Scored':'Points','Weightlifting_Sessions_Average':'WL','Yoga_Sessions_Average':'Yoga',
                        'Laps_Run_Per_Practice_Average':'Laps','Water_Intake':'WI',
                        'Players_Absent_For_Sessions':'PAFS'})

```

---

```python
# top rows post renaming the columns
df.head()

```

<div style="display: flex; justify-content: center; width: 100%; overflow-x: auto;">
<div style="max-width: 100%;">
<style scoped>
    .dataframe {
        margin: 0 auto;  /* Centers the table */
        width: auto;     /* Allows table to size to content */
        max-width: 100%; /* Prevents table from overflowing container */
    }
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: center;
        white-space: nowrap; /* Prevents header text from wrapping */
    }
    .dataframe tbody td {
        white-space: nowrap; /* Prevents cell text from wrapping */
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Team</th>
      <th>Points</th>
      <th>WL</th>
      <th>Yoga</th>
      <th>Laps</th>
      <th>WI</th>
      <th>PAFS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Lakers</td>
      <td>242</td>
      <td>23.0</td>
      <td>25.0</td>
      <td>30.0</td>
      <td>5.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Lakers</td>
      <td>144</td>
      <td>24.0</td>
      <td>26.0</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Lakers</td>
      <td>156</td>
      <td>24.0</td>
      <td>27.0</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Lakers</td>
      <td>159</td>
      <td>26.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Lakers</td>
      <td>106</td>
      <td>27.0</td>
      <td>NaN</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>6.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

---

### **Distribution Plots**

- Distribution plots are graphical representations that show the distribution of a set of numerical data. These plots are used to gain insight into the characteristics of the data, such as the central tendency, spread, and skewness. There are several types of distribution plots, including histograms, density plots, box plots, and violin plots.

- A histogram is a bar graph that represents the frequency distribution of a set of data. It shows how many data points fall into each range of values or bin. The bars in the histogram represent the frequency of data points within a given range, and the height of each bar represents the number of data points in that bin.

- A density plot is a smoothed representation of the distribution of the data, which is calculated by fitting a probability density function to the histogram of the data. It shows the shape of the distribution and provides a visual representation of the relative density of the data at different values.

- A box plot, also known as a box-and-whisker plot, is a graphical representation of the distribution of a set of data. It shows the median, quartiles, and outliers of the data in a compact and easily interpretable format. The box in the plot represents the interquartile range (IQR), which is the range between the first and third quartile. The whiskers extend from the box to the minimum and maximum values of the data, and any outliers are plotted as individual points outside the whiskers.

- A violin plot is a combination of a density plot and a box plot, showing the density of the data along the y-axis and the distribution along the x-axis. It shows the distribution of the data in a compact format, and provides information on the central tendency, spread, and skewness of the data.

- Distribution plots are an important tool for exploratory data analysis and can help in understanding the distribution of the data, identifying patterns and outliers, and making informed decisions about the data. They provide a visual representation of the data and can be used to identify potential issues with the data, such as non-normality or outliers.

---

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and Points is the column you're plotting
plt.figure(figsize=(8, 6), dpi=80)
sns.displot(df.Points, kde=True)
plt.xlabel("Points")
plt.ylabel("Density")
plt.title("Distribution Plot for Points")
plt.show()

```

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/plots1.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
#boxplot visualization
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(8, 6), dpi=80)
plt.boxplot(df.Points)
plt.title("Box Plot for Points")

```

---

```python
df.tail(100)

```

---

<div style="display: flex; justify-content: center; width: 100%; overflow-x: auto;">
<div style="max-width: 100%;">
<style scoped>
    .dataframe {
        margin: 0 auto;  /* Centers the table */
        width: auto;     /* Allows table to size to content */
        max-width: 100%; /* Prevents table from overflowing container */
    }
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: center;
        white-space: nowrap; /* Prevents header text from wrapping */
    }
    .dataframe tbody td {
        white-space: nowrap; /* Prevents cell text from wrapping */
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Team</th>
      <th>Points</th>
      <th>WL</th>
      <th>Yoga</th>
      <th>Laps</th>
      <th>WI</th>
      <th>PAFS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>59</th>
      <td>Clippers</td>
      <td>156</td>
      <td>34.0</td>
      <td>28.0</td>
      <td>40.0</td>
      <td>5.0</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>60</th>
      <td>Clippers</td>
      <td>122</td>
      <td>37.0</td>
      <td>29.0</td>
      <td>44.0</td>
      <td>5.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>61</th>
      <td>Warriors</td>
      <td>55</td>
      <td>14.0</td>
      <td>8.0</td>
      <td>17.0</td>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>62</th>
      <td>Warriors</td>
      <td>60</td>
      <td>14.0</td>
      <td>8.0</td>
      <td>17.0</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>63</th>
      <td>Warriors</td>
      <td>90</td>
      <td>16.0</td>
      <td>11.0</td>
      <td>20.0</td>
      <td>3.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>154</th>
      <td>Bulls</td>
      <td>12</td>
      <td>12.0</td>
      <td>12.0</td>
      <td>13.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>155</th>
      <td>Bulls</td>
      <td>13</td>
      <td>12.0</td>
      <td>12.0</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>156</th>
      <td>Bulls</td>
      <td>12</td>
      <td>12.0</td>
      <td>13.0</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>157</th>
      <td>Bulls</td>
      <td>20</td>
      <td>13.0</td>
      <td>14.0</td>
      <td>15.0</td>
      <td>1.0</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>158</th>
      <td>Bulls</td>
      <td>0</td>
      <td>14.0</td>
      <td>15.0</td>
      <td>16.0</td>
      <td>1.0</td>
      <td>5.0</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 7 columns</p>
</div>

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/plot2.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
#function for plotting violin plots
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

def plotting_box_violin_plots(df,x,y):
  fig,axes=plt.subplots(1,2,figsize=(18,10))
  fig.suptitle("Violin and box plots for variable : {}".format(y))

  sns.violinplot(ax=axes[0],x=x,y=y,data=df)
  sns.boxplot(ax=axes[1],data=df[y])

  axes[0].set_title("Violin plot for variable : {}".format(y))
  axes[1].set_title("Box plot for variable : {}".format(y))

```

---

```python
for x in ['WL','Yoga','Laps','WI','PAFS']:
  plotting_box_violin_plots(df,"Team",x)

```

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/p1.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/p2.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/p3.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/p4.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/p5.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

### **Finding Outliers using Inter Quartile Range**

- The Interquartile Range (IQR) is a commonly used method for identifying outliers in a dataset. The IQR is calculated as the difference between the 75th and 25th percentiles of the data, and outliers are considered to be any observations that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR. This method is based on the assumption that the data is approximately normally distributed.

- The IQR is a robust statistic, meaning that it is not greatly affected by outliers in the data. This makes it a useful tool for finding outliers, particularly in datasets that contain a high degree of variability. The IQR method is a simple way of detecting outliers in a dataset and can be easily implemented in most data analysis software.

- By using IQR to identify outliers, it is possible to obtain a more accurate representation of the central tendency and variability of the data, which can be used to make more informed decisions about the data. It can also help to identify potential data errors or outliers that may be affecting the results of subsequent analysis.

---

```python
#function for finding out outliers
def find_outliers(df,column):
  Q1=df[column].quantile(0.25)
  Q3=df[column].quantile(0.75)
  IQR=Q3-Q1
  Upper_End=Q3+1.5*IQR
  Lower_End=Q1-1.5*IQR

  outlier=df[column][(df[column]>Upper_End)| (df[column]<Lower_End) ]

  return outlier

```

---

```python
for column in ['WL','Yoga','Laps','WI','PAFS']:
  print('\n Outliers in column "%s"' %column)

  outlier= find_outliers(df,column)
  print(outlier)

```

---

``` txt

Outliers in column "WL"
8      1111111.0
142         56.0
143         56.0
144         59.0
Name: WL, dtype: float64

 Outliers in column "Yoga"
140    52.0
141    56.0
142    60.0
143    60.0
144    63.0
Name: Yoga, dtype: float64

 Outliers in column "Laps"
144    68.0
Name: Laps, dtype: float64

 Outliers in column "WI"
Series([], Name: WI, dtype: float64)

 Outliers in column "PAFS"
Series([], Name: PAFS, dtype: float64)

```

---

```python
#removing outliers
df_clean=df.drop([142,143,144])

```

---

```python
df_clean.shape

```

(156, 7)

---

```python
df_clean['WL'][df_clean['WL']==1111111.0]=np.nan

```

---

```python
df_clean['WL']

```

0      23.0
1      24.0
2      24.0
3      26.0
4      27.0
       ... 
154    12.0
155    12.0
156    12.0
157    13.0
158    14.0
Name: WL, Length: 156, dtype: float64

---

### **Think about it - I**

- Are there any assumptions or limitations associated with using the IQR method to identify outliers?

- Can you think of other methods for identifying and dealing with outliers in a dataset?

---

## **Imputation Techniques**

- Data imputation refers to the process of filling in missing values in a dataset. In machine learning, imputing missing values is important because many algorithms are sensitive to missing values and cannot handle them effectively. If missing values are not handled, they can lead to biased or incorrect results, especially when using methods like regression, decision trees, and clustering.

- There are several techniques used for data imputation in machine learning, including:

**Mean/Median/Mode imputation**: In this method, missing values are replaced by the mean, median, or mode of the non-missing values in the same column. Mean imputation is used for continuous variables, median imputation for ordinal variables, and mode imputation for categorical variables. This method is simple and easy to implement, but it can result in loss of information and introduce bias if the distribution of the data is not symmetrical.

**Regression imputation**: In this method, a regression model is used to predict the missing values based on the values of the other variables in the dataset. The regression model can be linear, polynomial, or any other type of regression model. The advantage of this method is that it can capture the relationship between the variables, but it requires a well-defined regression model and can result in overfitting if the model is too complex.

---

```python
ncounts=pd.DataFrame([df_clean.isna().mean()]).T

```

---

```python
ncounts=ncounts.rename(columns={1:'data_missing'})

ncounts

```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Team</th>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Points</th>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>WL</th>
      <td>0.064103</td>
    </tr>
    <tr>
      <th>Yoga</th>
      <td>0.128205</td>
    </tr>
    <tr>
      <th>Laps</th>
      <td>0.089744</td>
    </tr>
    <tr>
      <th>WI</th>
      <td>0.089744</td>
    </tr>
    <tr>
      <th>PAFS</th>
      <td>0.185897</td>
    </tr>
  </tbody>
</table>
</div>

---

```python
#plot of missing value in each column
ncounts.plot(kind='barh',title='% of missing values across each column')


```

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/barh.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
####
# 1. Pandas Way -> F fill and B fill, groupby 
# 2. Sklearn -> Imputer -> Simple, Iterative, KNN
# 3. LGBM -> To fill in the missing values


```

---

```python
df_clean.shape, df_clean.dropna(axis=0).shape


```

((156, 7), (94, 7))

---

### Lets start with Pandas Imputer

- Pandas provides various methods for data imputation, including forward fill (ffill), backward fill (bfill), and group-by imputation.

- ffill imputes missing values with the value from the previous observation in the same column.

- bfill imputes missing values with the value from the next observation in the same column.

- groupby imputation is a more advanced technique where the missing values are imputed based on the values from other observations that are in the same group. This technique is often used when the missing values are not missing at random and are related to other variables in the data.

---

```python
df_clean.info()


```

``` txt
<class 'pandas.core.frame.DataFrame'>
Int64Index: 156 entries, 0 to 158
Data columns (total 7 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   Team    156 non-null    object 
 1   Points  156 non-null    int64  
 2   WL      146 non-null    float64
 3   Yoga    136 non-null    float64
 4   Laps    142 non-null    float64
 5   WI      142 non-null    float64
 6   PAFS    127 non-null    float64
dtypes: float64(5), int64(1), object(1)
memory usage: 9.8+ KB

```

---

```python
df_clean.info()


```

0      23.0
1      24.0
2      24.0
3      26.0
4      27.0
       ... 
154    12.0
155    12.0
156    12.0
157    13.0
158    14.0
Name: WL, Length: 156, dtype: float64

---

```python
#visualizing after filling missing value with mean
figure(figsize=(8, 6), dpi=80)
sns.distplot(df_clean['WL'].fillna(df_clean['WL'].mean()))
plt.xlabel("WL")
plt.ylabel("Density")
plt.title("Distribution Plot for WL")

```

Text(0.5, 1.0, 'Distribution Plot for WL')

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/barh2.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
#visualizing after filling missing value with median
figure(figsize=(8, 6), dpi=80)
sns.distplot(df_clean['WL'].fillna(df_clean['WL'].median()))
plt.xlabel("WL")
plt.ylabel("Density")
plt.title("Distribution Plot for WL")

```

Text(0.5, 1.0, 'Distribution Plot for WL')

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/barh3.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
#Groupby team
mean_WL=df_clean.groupby("Team")['WL'].mean().to_dict()

```

---

```python
mean_WL

```

{'Bulls': 11.214285714285714,
 'Clippers': 29.0,
 'Lakers': 26.04255319148936,
 'Porcupines': 39.42857142857143,
 'Trailblazers': 25.425925925925927,
 'Warriors': 18.727272727272727}

---

- The code iterates over each row in the 'df_clean' DataFrame. For each row, it checks if the 'WL' value is NaN. If it is, it retrieves the corresponding team value and looks up the mean value from the 'mean_WL' dictionary using the team as the key. Then, it updates the 'WL' value in the DataFrame at that specific index with the retrieved mean value. This process ensures that NaN values in the 'WL' column are replaced with the mean value specific to their corresponding team.

```python
for index, row in df_clean.iterrows():
    team = row['Team']
    if pd.isna(row['WL']):
        mean_value = mean_WL.get(team)
        df_clean.at[index, 'WL'] = mean_value
```

---

- The replace function cannot be directly used in this case because it does not support conditional replacements based on different values for each row. Here In this specific case, the replacement of NaN values in the 'WL' column depends on the value of the 'Team' column for each row. Since the replacement value varies for each row (based on the team).

- Also replace function is used in the case of strings, here WL is a float and we need to replace it by np.nan which is also float, that is why we can use either fillna as we have shown later as an alternate option or iterate as we did above.

---

```python
figure(figsize=(8, 6), dpi=80)
sns.distplot(df_clean['WL'].replace(mean_WL))
plt.xlabel("WL")
plt.ylabel("Density")
plt.title("Distribution Plot for WL")
```

Text(0.5, 1.0, 'Distribution Plot for WL')

---

</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/barh5.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
df_clean['WL'].replace(mean_WL)
```

0      23.0
1      24.0
2      24.0
3      26.0
4      27.0
       ... 
154    12.0
155    12.0
156    12.0
157    13.0
158    14.0
Name: WL, Length: 156, dtype: float64

---

```python
#you can use this function too fillna
df_clean['WL']=df_clean.groupby('Team')['WL'].transform(lambda x:x.fillna(x.mean()))
```

---

### Sklearn Imputer

sklearn.impute is a module in scikit-learn library that contains different imputation techniques for handling missing values in datasets. The main imputers provided by this module are:

- SimpleImputer: This imputer is a basic and straightforward imputation technique that replaces missing values with the mean, median, or most frequent value of the column.

- IterativeImputer: This imputer replaces missing values by modeling each feature with missing values as a function of other features and then iteratively filling in missing values. This method works well when the missing values are missing at random and there is a high correlation between the features.

- KNN imputation is an advanced method for handling missing values. The basic idea behind KNN imputation is to fill missing values with the mean of the k-nearest neighbors to the missing value. This method requires that you choose a distance metric and the value of k. The advantage of KNN imputation is that it can preserve the correlations in the data, which is important for many machine learning algorithms.

- In summary, scikit-learn provides a variety of imputation techniques, each with its own strengths and weaknesses. Choose the imputation technique that works best for your specific dataset and problem.

---

```python
# 1 Simple Imputer

Features=['WL','Yoga','Laps','WI','PAFS']

from sklearn.impute import SimpleImputer
impt=SimpleImputer(strategy='mean')

#Fit & Transform

si_impt=impt.fit_transform(df_clean[Features])
si_impt_df=pd.DataFrame(si_impt,columns=Features)

si_impt_df
```

---

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WL</th>
      <th>Yoga</th>
      <th>Laps</th>
      <th>WI</th>
      <th>PAFS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>23.0</td>
      <td>25.000000</td>
      <td>30.0</td>
      <td>5.0</td>
      <td>4.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24.0</td>
      <td>26.000000</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>8.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.0</td>
      <td>27.000000</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>7.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>26.0</td>
      <td>29.000000</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27.0</td>
      <td>20.823529</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>151</th>
      <td>12.0</td>
      <td>12.000000</td>
      <td>13.0</td>
      <td>1.0</td>
      <td>4.417323</td>
    </tr>
    <tr>
      <th>152</th>
      <td>12.0</td>
      <td>12.000000</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>4.417323</td>
    </tr>
    <tr>
      <th>153</th>
      <td>12.0</td>
      <td>13.000000</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>4.417323</td>
    </tr>
    <tr>
      <th>154</th>
      <td>13.0</td>
      <td>14.000000</td>
      <td>15.0</td>
      <td>1.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>155</th>
      <td>14.0</td>
      <td>15.000000</td>
      <td>16.0</td>
      <td>1.0</td>
      <td>5.000000</td>
    </tr>
  </tbody>
</table>
<p>156 rows × 5 columns</p>
</div>

---

```python
ITI=IterativeImputer(max_iter=10)

#Fit & Transform

ITI_impt=ITI.fit_transform(df_clean[Features])

ITI_impt_df=pd.DataFrame(ITI_impt,columns=Features)

ITI_impt_df

```

---

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WL</th>
      <th>Yoga</th>
      <th>Laps</th>
      <th>WI</th>
      <th>PAFS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>23.0</td>
      <td>25.000000</td>
      <td>30.0</td>
      <td>5.0</td>
      <td>4.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24.0</td>
      <td>26.000000</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>8.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.0</td>
      <td>27.000000</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>7.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>26.0</td>
      <td>29.000000</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27.0</td>
      <td>22.462586</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>151</th>
      <td>12.0</td>
      <td>12.000000</td>
      <td>13.0</td>
      <td>1.0</td>
      <td>4.411891</td>
    </tr>
    <tr>
      <th>152</th>
      <td>12.0</td>
      <td>12.000000</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>4.412035</td>
    </tr>
    <tr>
      <th>153</th>
      <td>12.0</td>
      <td>13.000000</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>4.412172</td>
    </tr>
    <tr>
      <th>154</th>
      <td>13.0</td>
      <td>14.000000</td>
      <td>15.0</td>
      <td>1.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>155</th>
      <td>14.0</td>
      <td>15.000000</td>
      <td>16.0</td>
      <td>1.0</td>
      <td>5.000000</td>
    </tr>
  </tbody>
</table>
<p>156 rows × 5 columns</p>
</div>

---

```python
# KNN Imputer 

from sklearn.impute import KNNImputer

KNN=KNNImputer(n_neighbors=3)

#Fit & Transform

KNN_impt=KNN.fit_transform(df_clean[Features])

KNN_impt_df=pd.DataFrame(KNN_impt,columns=Features)

KNN_impt_df

```

---

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WL</th>
      <th>Yoga</th>
      <th>Laps</th>
      <th>WI</th>
      <th>PAFS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>23.0</td>
      <td>25.000000</td>
      <td>30.0</td>
      <td>5.0</td>
      <td>4.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24.0</td>
      <td>26.000000</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>8.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.0</td>
      <td>27.000000</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>7.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>26.0</td>
      <td>29.000000</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27.0</td>
      <td>22.666667</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>151</th>
      <td>12.0</td>
      <td>12.000000</td>
      <td>13.0</td>
      <td>1.0</td>
      <td>4.666667</td>
    </tr>
    <tr>
      <th>152</th>
      <td>12.0</td>
      <td>12.000000</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>5.333333</td>
    </tr>
    <tr>
      <th>153</th>
      <td>12.0</td>
      <td>13.000000</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>154</th>
      <td>13.0</td>
      <td>14.000000</td>
      <td>15.0</td>
      <td>1.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>155</th>
      <td>14.0</td>
      <td>15.000000</td>
      <td>16.0</td>
      <td>1.0</td>
      <td>5.000000</td>
    </tr>
  </tbody>
</table>
<p>156 rows × 5 columns</p>
</div>

---

### LGBM Imputer

LGBMImputer is a data imputation technique from the Python project kuma_utils https://github.com/analokmaus/kuma_utils.git, which is based on the LightGBM library. LGBMImputer is designed to fill in missing values in a dataset using gradient boosting decision trees. This technique is a variation of tree-based imputation, which uses regression trees to make predictions for missing values based on the values of other variables in the dataset.

The LGBMImputer class takes in a dataset and a list of columns to be imputed as inputs. It then fits a LightGBM model to the data, with the target variable being the missing values and the features being the remaining variables. After training, the model is used to predict the missing values.

One advantage of LGBMImputer is that it is able to handle both categorical and numerical data, unlike some other imputation techniques that are only suitable for one or the other. Additionally, LightGBM is a fast and efficient algorithm, so LGBMImputer is able to handle large datasets with many missing values quickly.

---

```python
from kuma_utils.preprocessing.imputer import LGBMImputer

```

---

```python
%%time
lgbm_itr=LGBMImputer(n_iter=100,verbose=True)

df_itr=lgbm_itr.fit_transform(df_clean[Features])


df_itr_df=pd.DataFrame(df_itr,columns=Features)

```

---

```python
%%time
df_itr_df

```

---

<!DOCTYPE html>
<html>
<head>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            font-family: Arial, sans-serif;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: center;
        }
        th {
            background-color: transparent;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: transparent;
        }
        tr:hover {
            background-color: #f1f1f1;
        }
    </style>
</head>
<body>

<h2>Data Table</h2>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: center;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th>WL</th>
      <th>Yoga</th>
      <th>Laps</th>
      <th>WI</th>
      <th>PAFS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>23.0</td>
      <td>25.000000</td>
      <td>30.0</td>
      <td>5.0</td>
      <td>4.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24.0</td>
      <td>26.000000</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>8.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.0</td>
      <td>27.000000</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>7.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>26.0</td>
      <td>29.000000</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27.0</td>
      <td>24.037313</td>
      <td>34.0</td>
      <td>5.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>154</th>
      <td>12.0</td>
      <td>12.000000</td>
      <td>13.0</td>
      <td>1.0</td>
      <td>3.697231</td>
    </tr>
    <tr>
      <th>155</th>
      <td>12.0</td>
      <td>12.000000</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>3.697231</td>
    </tr>
    <tr>
      <th>156</th>
      <td>12.0</td>
      <td>13.000000</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>3.949597</td>
    </tr>
    <tr>
      <th>157</th>
      <td>13.0</td>
      <td>14.000000</td>
      <td>15.0</td>
      <td>1.0</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>158</th>
      <td>14.0</td>
      <td>15.000000</td>
      <td>16.0</td>
      <td>1.0</td>
      <td>5.000000</td>
    </tr>
  </tbody>
</table>
<p>156 rows × 5 columns</p>
</div>

</body>
</html>


---

```python
df_new=df_itr_df

```

---

### **Univariate Analysis**

Univariate analysis is a statistical technique used to examine the distribution of a single variable. It is the simplest form of data analysis that helps to understand the patterns, central tendency, and spread of the data.

There are several types of univariate analysis techniques, including:

- **Histograms**: A histogram is a graph that represents the frequency distribution of a set of data. It provides a visual representation of how the data is spread over a range of values. The x-axis represents the range of values and the y-axis represents the frequency of those values.

- **Box Plots**: A box plot is a visual representation of the distribution of a set of data. It consists of a box that encloses the middle 50% of the data (the interquartile range), a line that represents the median, and two whiskers that extend from the box to the minimum and maximum values. Outliers are also indicated on the plot.

- **Distribution Plots**: Distribution plots display the shape of the distribution of a single variable. They help to visualize the skewness and kurtosis of the data, as well as the presence of any outliers or extreme values. Examples of distribution plots include kernel density plots, normal quantile plots, and probability plots.

Univariate analysis is an important step in the data exploration process, as it helps to identify any outliers, skewness, or patterns in the data that may impact further analysis or modeling.

---

```python
df_new.columns

```

---

> Index(['WL', 'Yoga', 'Laps', 'WI', 'PAFS'], dtype='object')

---

```python
#distribution plot
sns.distplot(np.sqrt(df_new["WL"]))

```

---

 <div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/c1.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>

</div>

---

```python
#boxplot
sns.boxplot(df_new["WL"])

```

---

 <div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/c2.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>

</div>

---

```python
#histogram plot
np.sqrt(df_new["WL"]).hist()
```

---

 <div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/c3.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>

</div>

---
