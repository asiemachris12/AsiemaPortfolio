---
layout: page
title: ML02. Linear Regression Model Project in Python for Beginners Part 2
description: with background image
img: assets/img/4.jpg
importance: 1
category: work
related_publications: true
---

The English Premier League is one of the world's most-watched soccer leagues, with an estimated audience of 12 million people per game.
With the substantial financial benefits, all significant teams of EPL are interested in Analytics and AI. Regarding sports analytics, machine learning and artificial intelligence (AI) have become extremely popular. The sports entertainment sector and the relevant stakeholders extensively use sophisticated algorithms to improve earnings and reduce business risk associated with selecting or betting on the wrong players.

---

![image](https://cdn.pixabay.com/photo/2016/04/15/20/28/football-1331838__340.jpg)

Regression is one of the foundational techniques in Machine Learning. As one of the most well-understood algorithms, linear regression plays a vital role in solving real-life problems.
In this project, we wish to use Linear Regression to predict the scores of EPL soccer players.
With the business implications cleared. Let's get into the project's technical details. This project is part of the Linear Regression Beginner Project Series, and it consists of discussing and implementing the fundamentals of Linear Regression in Python on the EPL Soccer Player Dataset.

---

## **Detailed Approach**

**SECTION ONE: SET UP THE ENVIRONNMENT**

* Install Packages

* Connect to Workspace

* Import Libraries

* Create experiment

* Create or Attach existing compute resource

**SECTION TWO: DATA ANALYSIS**

* Data Reading from Different Sources

* Exploratory Data Analysis

* Correlation

* Relationship between Cost and Score

* Train - Test Split

**SECTION THREE: MODEL TRAINING**

* Linear Regression

* Model Summary

* Prediction on Test Data

* Diagnostics and Remedies

---

## **STEP ONE: Set up your development environment**

All the setup for your development work can be accomplished in a Python notebook.  Setup includes:

* Importing Python packages

* Connecting to a workspace to enable communication between your local computer and remote resources

* Creating an experiment to track all your runs

* Creating a remote compute target to use for training

---

## **Important Libraries**

* **Pandas**: pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool built on top of the Python programming language. Refer to [documentation](https://pandas.pydata.org/) for more information.

* **NumPy**: The fundamental package for scientific computing with Python. Fast and versatile, the NumPy vectorization, indexing, and broadcasting concepts are the de-facto standards of array computing today. NumPy offers comprehensive mathematical functions, random number generators, linear algebra routines, Fourier transforms, and more. Refer to [documentation](https://numpy.org/) for more information. pandas and NumPy are together used for most of the data analysis and manipulation in Python.

* **Matplotlib**: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Refer to [documentation](https://matplotlib.org/) for more information.

* **Seaborn**: Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Refer to [documentation](https://seaborn.pydata.org/) for more information.

* **Scikit-learn**: Simple and efficient tools for predictive data analysis
accessible to everybody and reusable in various contexts.
It is built on NumPy, SciPy, and matplotlib to support machine learning in Python. Refer to [documentation](https://scikit-learn.org/stable/) for more information.

* **Statsmodels**: statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests and statistical data exploration. Refer to [documentation](https://www.statsmodels.org/stable/index.html) for more information.

* **SciPy**: SciPy provides algorithms for optimization, integration, interpolation, eigenvalue problems, algebraic equations, differential equations, statistics, and many other classes of problems. Refer to [documentation](https://scipy.org/) for more information.

---

## **Install Packages**

```python
import warnings
warnings.filterwarnings('ignore')
```

```python
# Install required packages with versions
%pip install numpy==1.21.2 --quiet
%pip install seaborn==0.11.1 --quiet
%pip install matplotlib==3.1.1 --quiet
%pip install statsmodels==0.12.2 --quiet
%pip install pandas==1.2.4 --quiet
%pip install scipy==1.6.3 --quiet
%pip install scikit_learn==1.0.2 --quiet
%pip install projectpro --upgrade --quiet
```

```python
# Install required packages with versions
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

import azureml.core
from azureml.core import Workspace

# check core SDK version number
print("Azure ML SDK Version: ", azureml.core.VERSION)

```

---

## **Connect to workspace**

Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`.

<br>

```python
# load workspace configuration from the config.json file in the current folder.
ws = Workspace.from_config()
print(ws.name, ws.location, ws.resource_group, sep='\t')

```

> Output: WS000-CentralWorkSpace | southafricanorth | RG000-Central_RG

---

## **Create experiment**

Create an experiment to track the runs in your workspace. A workspace can have muliple experiments.

```python
experiment_name = 'TUT001-Reg_Part1'

from azureml.core import Experiment
exp = Experiment(workspace=ws, name=experiment_name)
print("Experiment name: ", exp.name)

```

> Experiment name: TUT001-Reg_Part1

---

## **Create or Attach existing compute resource**

By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. You will submit Python code to run on this VM later in the tutorial.

The code below creates the compute clusters for you if they don't already exist in your workspace.

**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process.

```python
from azureml.core.compute import AmlCompute
from azureml.core.compute import ComputeTarget
import os

# choose a name for your cluster
compute_name = os.environ.get("AML_COMPUTE_CLUSTER_NAME", "cpu-cluster")
compute_min_nodes = os.environ.get("AML_COMPUTE_CLUSTER_MIN_NODES", 0)
compute_max_nodes = os.environ.get("AML_COMPUTE_CLUSTER_MAX_NODES", 4)

# This example uses CPU VM. For using GPU VM, set SKU to Standard_NC6s_v3
vm_size = os.environ.get("AML_COMPUTE_CLUSTER_SKU", "STANDARD_D2_V2")


if compute_name in ws.compute_targets:
    compute_target = ws.compute_targets[compute_name]
    if compute_target and type(compute_target) is AmlCompute:
        print("found compute target: " + compute_name)
else:
    print("creating new compute target...")
    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,
                                                                min_nodes = compute_min_nodes, 
                                                                max_nodes = compute_max_nodes)

    # create the cluster
    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)
    
    # can poll for a minimum number of nodes and for a specific timeout. 
    # if no min node count is provided it will use the scale settings for the cluster
    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)
    
    # For a more detailed view of current AmlCompute status, use get_status()
    print(compute_target.get_status().serialize())

```

You now have the necessary packages and compute resources to train a model in the cloud.

```python
#import required packages

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from scipy import stats
import scipy
from matplotlib.pyplot import figure
```

```python
# Load the data as a data frame by using URL
df = pd.read_csv('EPL_Soccer_MLR_LR.csv')
```

<div style="display: flex; justify-content: center; width: 100%; overflow-x: auto;">
<div style="max-width: 100%;">
<style scoped>
    .dataframe {
        margin: 0 auto;  /* Centers the table */
        width: auto;     /* Allows table to size to content */
        max-width: 100%; /* Prevents table from overflowing container */
    }
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: center;
        white-space: nowrap; /* Prevents header text from wrapping */
    }
    .dataframe tbody td {
        white-space: nowrap; /* Prevents cell text from wrapping */
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>PlayerName</th>
      <th>Club</th>
      <th>DistanceCovered(InKms)</th>
      <th>Goals</th>
      <th>MinutestoGoalRatio</th>
      <th>ShotsPerGame</th>
      <th>AgentCharges</th>
      <th>BMI</th>
      <th>Cost</th>
      <th>PreviousClubCost</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Braund, Mr. Owen Harris</td>
      <td>MUN</td>
      <td>3.96</td>
      <td>7.5</td>
      <td>37.5</td>
      <td>12.3</td>
      <td>60</td>
      <td>20.56</td>
      <td>109.1</td>
      <td>63.32</td>
      <td>195.9</td>
      <td>78.9</td>
      <td>19.75</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Allen, Mr. William Henry</td>
      <td>MUN</td>
      <td>4.41</td>
      <td>8.3</td>
      <td>38.2</td>
      <td>12.7</td>
      <td>68</td>
      <td>20.67</td>
      <td>102.8</td>
      <td>58.55</td>
      <td>189.7</td>
      <td>74.4</td>
      <td>21.30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Moran, Mr. James</td>
      <td>MUN</td>
      <td>4.14</td>
      <td>5.0</td>
      <td>36.4</td>
      <td>11.6</td>
      <td>21</td>
      <td>21.86</td>
      <td>104.6</td>
      <td>55.36</td>
      <td>177.8</td>
      <td>69.1</td>
      <td>19.88</td>
    </tr>
    <tr>
      <th>3</th>
      <td>McCarthy, Mr. Timothy J</td>
      <td>MUN</td>
      <td>4.11</td>
      <td>5.3</td>
      <td>37.3</td>
      <td>12.6</td>
      <td>69</td>
      <td>21.88</td>
      <td>126.4</td>
      <td>57.18</td>
      <td>185.0</td>
      <td>74.9</td>
      <td>23.66</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Palsson, Master. Gosta Leonard</td>
      <td>MUN</td>
      <td>4.45</td>
      <td>6.8</td>
      <td>41.5</td>
      <td>14.0</td>
      <td>29</td>
      <td>18.96</td>
      <td>80.3</td>
      <td>53.20</td>
      <td>184.6</td>
      <td>64.6</td>
      <td>17.64</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>197</th>
      <td>Ryan, Mr. Patrick</td>
      <td>LIV</td>
      <td>4.90</td>
      <td>7.6</td>
      <td>45.6</td>
      <td>16.0</td>
      <td>90</td>
      <td>27.56</td>
      <td>67.2</td>
      <td>82.00</td>
      <td>183.9</td>
      <td>93.2</td>
      <td>11.79</td>
    </tr>
    <tr>
      <th>198</th>
      <td>Saad, Mr. Amin</td>
      <td>LIV</td>
      <td>5.66</td>
      <td>8.3</td>
      <td>50.2</td>
      <td>17.7</td>
      <td>38</td>
      <td>23.76</td>
      <td>56.5</td>
      <td>72.00</td>
      <td>183.5</td>
      <td>80.0</td>
      <td>10.05</td>
    </tr>
    <tr>
      <th>199</th>
      <td>Saad, Mr. Khalil</td>
      <td>LIV</td>
      <td>5.03</td>
      <td>6.4</td>
      <td>42.7</td>
      <td>14.3</td>
      <td>122</td>
      <td>22.01</td>
      <td>47.6</td>
      <td>68.00</td>
      <td>183.1</td>
      <td>73.8</td>
      <td>8.51</td>
    </tr>
    <tr>
      <th>200</th>
      <td>Saade, Mr. Jean Nassr</td>
      <td>LIV</td>
      <td>4.97</td>
      <td>8.8</td>
      <td>43.0</td>
      <td>14.9</td>
      <td>233</td>
      <td>22.34</td>
      <td>60.4</td>
      <td>63.00</td>
      <td>178.4</td>
      <td>71.1</td>
      <td>11.50</td>
    </tr>
    <tr>
      <th>201</th>
      <td>Sadlier, Mr. Matthew</td>
      <td>LIV</td>
      <td>5.38</td>
      <td>6.3</td>
      <td>46.0</td>
      <td>15.7</td>
      <td>32</td>
      <td>21.07</td>
      <td>34.9</td>
      <td>72.00</td>
      <td>190.8</td>
      <td>76.7</td>
      <td>6.26</td>
    </tr>
  </tbody>
</table>
<p>202 rows × 13 columns</p>
</div>
</div>
---

---

```python
df.columns
```

> Index(['PlayerName', 'Club', 'DistanceCovered(InKms)', 'Goals','MinutestoGoalRatio', 'ShotsPerGame', 'AgentCharges', 'BMI', 'Cost', 'PreviousClubCost', 'Height', 'Weight', 'Score'], dtype='object')

---

## **Data Dictionary**

* PlayerName : Player Name

* Club : Club of the player
  1. MUN:Manchester United F.C.
  2. CHE: Chelsea F.C.
  3. LIV: Liverpool F.C.

* DistanceCovered(InKms): Average Kms distance covered by the player in each game

* Goals: Average Goals per match

* MinutestoGoalRatio: Minutes

* ShotsPerGame: Average shots taken per game

* AgentCharges: Agent Fees in h

* BMI: Body-Mass index

* Cost: Cost of each player in hundread thousand dollars

* PreviousClubCost: Previous club cost in hundread thousand dollars

* Height: Height of player in cm

* Weight: Weight of player in kg

* Score: Average score per match

---

## **Exploratory Data Analysis**

Exploratory Data Analysis, commonly known as EDA, is a technique to analyze the data with visuals. It involves using statistics and visual techniques to identify particular trends in data.

It is used to understand data patterns, spot anomalies, check assumptions, etc. The main purpose of EDA is to help look into the data before making any hypothesis about it.

---

### **Dataframe Information**

To generate descriptive statistics [pandas.dataframe.describe()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function is used.

Descriptive statistics include those that summarize the central tendency, dispersion, and shape of a dataset’s distribution, excluding NaN values.

---

```python
df.shape
```

> (202, 13)

---

```python
# Get basic description of the data, looking the spread of the different variables,
# along with  abrupt changes between the minimum, 25th, 50th, 75th, and max for the different variables
df.describe()
```

<div style="display: flex; justify-content: center; width: 100%; overflow-x: auto;">
<div style="max-width: 100%;">
<style scoped>
    .dataframe {
        margin: 0 auto;  /* Centers the table */
        width: auto;     /* Allows table to size to content */
        max-width: 100%; /* Prevents table from overflowing container */
    }
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: center;
        white-space: nowrap; /* Prevents header text from wrapping */
    }
    .dataframe tbody td {
        white-space: nowrap; /* Prevents cell text from wrapping */
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>DistanceCovered(InKms)</th>
      <th>Goals</th>
      <th>MinutestoGoalRatio</th>
      <th>ShotsPerGame</th>
      <th>AgentCharges</th>
      <th>BMI</th>
      <th>Cost</th>
      <th>PreviousClubCost</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
      <td>202.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>4.718614</td>
      <td>7.108663</td>
      <td>43.091584</td>
      <td>14.566337</td>
      <td>76.876238</td>
      <td>22.955891</td>
      <td>69.021782</td>
      <td>64.873713</td>
      <td>180.103960</td>
      <td>75.008168</td>
      <td>13.507426</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.457976</td>
      <td>1.800549</td>
      <td>3.662989</td>
      <td>1.362451</td>
      <td>47.501239</td>
      <td>2.863933</td>
      <td>32.565333</td>
      <td>13.070197</td>
      <td>9.734494</td>
      <td>13.925574</td>
      <td>6.189826</td>
    </tr>
    <tr>
      <th>min</th>
      <td>3.800000</td>
      <td>3.300000</td>
      <td>35.900000</td>
      <td>11.600000</td>
      <td>8.000000</td>
      <td>16.750000</td>
      <td>28.000000</td>
      <td>34.360000</td>
      <td>148.900000</td>
      <td>37.800000</td>
      <td>5.630000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>4.372500</td>
      <td>5.900000</td>
      <td>40.600000</td>
      <td>13.500000</td>
      <td>41.250000</td>
      <td>21.082500</td>
      <td>43.850000</td>
      <td>54.667500</td>
      <td>174.000000</td>
      <td>66.525000</td>
      <td>8.545000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>4.755000</td>
      <td>6.850000</td>
      <td>43.500000</td>
      <td>14.700000</td>
      <td>65.500000</td>
      <td>22.720000</td>
      <td>58.600000</td>
      <td>63.035000</td>
      <td>179.700000</td>
      <td>74.400000</td>
      <td>11.650000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>5.030000</td>
      <td>8.275000</td>
      <td>45.575000</td>
      <td>15.575000</td>
      <td>97.000000</td>
      <td>24.465000</td>
      <td>90.350000</td>
      <td>74.750000</td>
      <td>186.175000</td>
      <td>84.125000</td>
      <td>18.080000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>6.720000</td>
      <td>14.300000</td>
      <td>59.700000</td>
      <td>19.200000</td>
      <td>234.000000</td>
      <td>34.420000</td>
      <td>200.800000</td>
      <td>106.000000</td>
      <td>209.400000</td>
      <td>123.200000</td>
      <td>35.520000</td>
    </tr>
  </tbody>
</table>
</div>
</div>

---

## **Correlation**

The correlation coefficient measures the strength of the relationship between two variables. It indicates that as the value of one variable changes, the other variable changes in a specific direction with some magnitude. There are various ways to find a correlation between two variables, one of which is the Pearson correlation coefficient.
It measures the linear relationship between two continuous variables.

---

### **Graphs of Different Correlation Coefficients**

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/differentcorrelation.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

$ r = -1 $ indicates a perfect negative relationship between the variables

$ r = 0 $ indicates no relationship between the variables

$ r = 1 $ indicates a perfect positive relationship between the variables

```python
# Correlation matrix
corr = df.corr(numeric_only=True)
corr
```

<div style="display: flex; justify-content: center; width: 100%; overflow-x: auto;">
<div style="max-width: 100%;">
<style scoped>
    .dataframe {
        margin: 0 auto;  /* Centers the table */
        width: auto;     /* Allows table to size to content */
        max-width: 100%; /* Prevents table from overflowing container */
    }
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: center;
        white-space: nowrap; /* Prevents header text from wrapping */
    }
    .dataframe tbody td {
        white-space: nowrap; /* Prevents cell text from wrapping */
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>DistanceCovered(InKms)</th>
      <th>Goals</th>
      <th>MinutestoGoalRatio</th>
      <th>ShotsPerGame</th>
      <th>AgentCharges</th>
      <th>BMI</th>
      <th>Cost</th>
      <th>PreviousClubCost</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>DistanceCovered(InKms)</th>
      <td>1.000000</td>
      <td>0.147098</td>
      <td>0.924964</td>
      <td>0.888800</td>
      <td>0.250865</td>
      <td>0.299471</td>
      <td>-0.403004</td>
      <td>0.550975</td>
      <td>0.358854</td>
      <td>0.403743</td>
      <td>-0.493512</td>
    </tr>
    <tr>
      <th>Goals</th>
      <td>0.147098</td>
      <td>1.000000</td>
      <td>0.153333</td>
      <td>0.134721</td>
      <td>0.131973</td>
      <td>0.177032</td>
      <td>0.137131</td>
      <td>0.102734</td>
      <td>0.076958</td>
      <td>0.155844</td>
      <td>0.108114</td>
    </tr>
    <tr>
      <th>MinutestoGoalRatio</th>
      <td>0.924964</td>
      <td>0.153333</td>
      <td>1.000000</td>
      <td>0.950757</td>
      <td>0.258240</td>
      <td>0.320527</td>
      <td>-0.449135</td>
      <td>0.583375</td>
      <td>0.371192</td>
      <td>0.423699</td>
      <td>-0.532449</td>
    </tr>
    <tr>
      <th>ShotsPerGame</th>
      <td>0.888800</td>
      <td>0.134721</td>
      <td>0.950757</td>
      <td>1.000000</td>
      <td>0.308391</td>
      <td>0.382524</td>
      <td>-0.435429</td>
      <td>0.610986</td>
      <td>0.352322</td>
      <td>0.455255</td>
      <td>-0.531522</td>
    </tr>
    <tr>
      <th>AgentCharges</th>
      <td>0.250865</td>
      <td>0.131973</td>
      <td>0.258240</td>
      <td>0.308391</td>
      <td>1.000000</td>
      <td>0.302556</td>
      <td>-0.108243</td>
      <td>0.317581</td>
      <td>0.123255</td>
      <td>0.273686</td>
      <td>-0.183386</td>
    </tr>
    <tr>
      <th>BMI</th>
      <td>0.299471</td>
      <td>0.177032</td>
      <td>0.320527</td>
      <td>0.382524</td>
      <td>0.302556</td>
      <td>1.000000</td>
      <td>0.321116</td>
      <td>0.713858</td>
      <td>0.337097</td>
      <td>0.845955</td>
      <td>0.187558</td>
    </tr>
    <tr>
      <th>Cost</th>
      <td>-0.403004</td>
      <td>0.137131</td>
      <td>-0.449135</td>
      <td>-0.435429</td>
      <td>-0.108243</td>
      <td>0.321116</td>
      <td>1.000000</td>
      <td>-0.207749</td>
      <td>-0.071253</td>
      <td>0.154227</td>
      <td>0.963017</td>
    </tr>
    <tr>
      <th>PreviousClubCost</th>
      <td>0.550975</td>
      <td>0.102734</td>
      <td>0.583375</td>
      <td>0.610986</td>
      <td>0.317581</td>
      <td>0.713858</td>
      <td>-0.207749</td>
      <td>1.000000</td>
      <td>0.802119</td>
      <td>0.930904</td>
      <td>-0.361850</td>
    </tr>
    <tr>
      <th>Height</th>
      <td>0.358854</td>
      <td>0.076958</td>
      <td>0.371192</td>
      <td>0.352322</td>
      <td>0.123255</td>
      <td>0.337097</td>
      <td>-0.071253</td>
      <td>0.802119</td>
      <td>1.000000</td>
      <td>0.780906</td>
      <td>-0.188022</td>
    </tr>
    <tr>
      <th>Weight</th>
      <td>0.403743</td>
      <td>0.155844</td>
      <td>0.423699</td>
      <td>0.455255</td>
      <td>0.273686</td>
      <td>0.845955</td>
      <td>0.154227</td>
      <td>0.930904</td>
      <td>0.780906</td>
      <td>1.000000</td>
      <td>-0.000162</td>
    </tr>
    <tr>
      <th>Score</th>
      <td>-0.493512</td>
      <td>0.108114</td>
      <td>-0.532449</td>
      <td>-0.531522</td>
      <td>-0.183386</td>
      <td>0.187558</td>
      <td>0.963017</td>
      <td>-0.361850</td>
      <td>-0.188022</td>
      <td>-0.000162</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
---

---

### **Correlation Heatmap**
A correlation heatmap is a heatmap that depicts a two-dimentional correlation matrix between two discrete dimensions, with colored pixels representing data on a color scale. The values of the first dimension are displayed as rows in the table, while the values of the second dimension appear as columns. The cell's hue is propotional to the number of measurements corresponding to the dimensional value.

This makes correlation heatmaps great for data analysis since they show differences and variances in the same data while makinf patterns clearly accessible. A correlation heatmap, like a standard heatmap, is aided by a color bar to make data more legible and understandable.

---

#### **Understanding Heatmap Parameters**

Other than data, all other parameters are optional, and data will clearly be the data to be plotted. To build a correlation heatmap, the data must be given using the corr() function.

**data: rectangular dataset**

2D dataset that can be coerced into an ndarray. If a Pandas DataFrame is provided, the index/column information will be used to label the columns and rows.

**vmin, vmax: floats, optional**

Values to anchor the colormap; otherwise, they are inferred from the data and other keyword arguments.

**cmap: matplotlib colormap name or object, or list of colors, optional**

The mapping from data values to color space. If not provided, the default will depend on whether the center is set.

**annot: bool or rectangular dataset, optional**

If True, write the data value in each cell. If an array-like with the same shape as the data, then use this to annotate the heatmap instead of the data. Note that DataFrames will match on position, not index.

**square: bool, optional**

If True, set the Axes aspect to “equal” so each cell will be square-shaped.

For heatmaps, refer to the [seaborn documentation](https://seaborn.pydata.org/generated/seaborn.heatmap.html).

To give your project a background in the portfolio page, just add the img tag to the front matter like so:

```python
import plotly.express as px
import pandas as pd
import numpy as np


# Calculate correlation matrix
corr = df.corr(numeric_only= True)

# Create Plotly heatmap
fig = px.imshow(
    corr,
    text_auto=True,  # Annotates cells with correlation values
    color_continuous_scale=px.colors.diverging.RdBu,  # Red-Blue diverging scale
    zmin=-1,  # Correlation ranges from -1
    zmax=1,  # To +1
    labels=dict(color="Correlation"),  # Add color bar label
)

# Update layout for readability
fig.update_layout(
    title="Correlation Heatmap",  # Title of the heatmap
    title_font_size=20,  # Font size of the title
    xaxis=dict(
        tickangle=45,  # Rotate x-axis labels
        title="Features",  # Add x-axis title
    ),
    yaxis=dict(title="Features"),  # Add y-axis title
    height=600,  # Chart height
    width=800,  # Chart width
    template="plotly_white",  # Use a clean white template
)

# Show plot
fig.show()

```

Let's analyze the correlation scores of variables concerning "Score."
* We should remove some weakly correlated variables such as height and Weight with -0.190 and 0.00016 correlation.

* Notice some variables are correlated with each other as well, such as MinutestoGoalRatio and ShortsPerGame. We will take only one of them, ShortsPerGame. What will happen if we include multiple such variables in our model? This is called multicollinearity, and we will be discussing it in detail.

---

## **Multicollinearity**

Some of the independent variables in regression are truly correlated with one another; this is known as multicollinearity, and it is critical to examine these before creating the regression model.

When two or more independent continuous variables in a dataset are highly correlated, they can help predict each other and the dependent variable. This makes analyzing the effect of these distinct, independent factors on the goal or dependent variable challenging. The model becomes sensitive to small changes in the data as changes in one multicollinear variable also show an inflated effect on others.

---

**How to calculate VIF?**

In the least squares regression models, variance inflation factors (VIFs) evaluate the correlation between independent variables. We can quantify multicollinearity using Variance Infaltion Factor (VIF). VIF determines the strength of the correlation between the independent variables. It is predicted by regressing a variable against every other variable.

To compute the VIFs, all independent variables are converted into dependent variables. Each model generates an R-squared number representing the precentage of variation in the individual IV that the group of IVs explains. As a result, larger R-squared values suggest greater multicollinearity. These R-squared values are used in VIF calculations

$$VIF = \frac{1}{1-R^2}$$ 

The more the value of R2 is closer to 1, the more the VIF score tends to infinity.

* VIF starts with one and denotes that the varable does not correlate at all.
* VIF more than 5-10 can be considered a serious case of multicollinearity and can affect prediction models.

If two independent variables are too highly correlated $R^2$ > ~0.5, then only one of them should be used in the regression model.

---

**Ways to handle multicollinearity**

* Remove some of the independent variables that are highly connected.

* Linearly combine the independent variables, for example, by adding them all together.

* Conduct a highly correlated variable study and dimensionality reduction techniques, such as principle components analysis or partial least squares regression.

* LASSO and Ridge regression are sophisticated regression analysis techniques that can deal with multicollinearity. You'll be able to manage these analyses with only a little more study if you know how to do linear least squares regression.

Since we are at the very beginning of our learning journey we will introduce the concepts in detail much later.

---

## **Multiple Linear Regression**

Linear regreession, often known as simple regression, creates a relationship between two variables. Linear regression is represented visually as straignt line, with the slope determining how a change in one variable affects a change in the other. A linear regression relationshi's y-intercept reflects one variable's value when the other's value is 0.

Multiple linear regression estimates the relationship between two or more independent variables and one dependent variable

$$\hat y = \beta_0+\beta_1x_1+\dots \beta_px_p+\epsilon$$

where $p$ is... number of features in the model.

* For any given independent variable (x) value, y is the dependent variable's predicted value for the dependent variable (y).

* $\beta_0$ represents the intercept, or the predicted value of y, when x is 0.

* $\beta_1$ is the regression coefficient of variable $x_1$, which tells us how much y will change as $x_1$ increases or decreases.

* $\beta_p$ is the regression coefficient of the last variable $x_p$, which tells us how much y will change as $x_p$ increases or decreases.

* $x_1$ ... $x_p$ are the independent or predictor variables that help us predict y

* $\epsilon$ is the error left due to the incorrect calculation of the regression coefficients.

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/linear.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

## **Errors in Regression**

The regression line regress towards the mean to create the best fit which essentially means that the errors are at the lowest. In the above plot, it is visible that the regression line is not able to exactly predict the true values. There is always going to be some space for errors.

Let's understand the various errors in Regression:

The mean aboslute error (MAE) is the most basic regression error statistic to grasp. We'll compute the residual for each data point individually, using only the absolute value of each so that negative and positive residuals don't cancel out. The average of all theses residuals is the calculated. MAE essentially describes the typical magnitude of the residuals

<br>

$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y-\hat y|$$

* The mean square error (MSE) is identical to the mean absolute error (MAE) but squares the difference before aggregating all of them. The MSE will nearly always be greater than the MAE because we are squaring the difference. Because of this, we are unable to directly compare the MAE and MSE. We are limited to comparing the error metrics of our model to those of a rival model. The presence of outliers in our data makes the square terms's impact on the MSE equation very clear. In MAE, each residual adds proportionally to the overall error, whereas in MSE, the error increases quadratically.

* As a result, our data outliers will untimately result in a considerably bigger total error in the MSE than they will in the MAE. Similarly to this, our model will suffer more if it predicts values that are signifficantly different from the matching actual value. This means that in MSE as opposed to MAE, substantial disparities between actual and predicted values are punished more serverely.

* If we wish to limit the importance of outliers, we should use MAE because outlier residuals do not contribute as much to overall error as MSE. Finally, the dissision between MSE and MAE is application-specific and depends on how large errors need to be handled

$$MSE= \frac{1}{n}\sum_{i=1}^{n}(y-\hat y)^2$$

* The root mean squared error (RMSE) is another error statistic you may come upon. It is the square root of the MSE, as the name implies. Becasuethe MSE is squared, its units differ from the original output. RMSE is frequently used to transform the error metric back into comparable units, making interpretation easier. Outliers have a comparable effects on the MSE and RMSE because they both square the residual.

$$RMSE= \sqrt(\frac{1}{n}\sum_{i=1}^{n}(y-\hat y)^2)$$

* The percentage counterpart of MAE is the mean absolute percentage error (MAPE). Just as MAE is the average amount of error created by your model, MAPE is the average distance between the model's predictions and their associated outputs. MAPE, like MAE, has a clear meaning because percentages are easier for people to understand. Because of the use of absolute value, MAPE and MAE are both resistant to the effects of outliers.

$$MAPE= \frac{100\%}{n}\ \sum_{i=1}^{n}\left| \frac{y-\hat y}{y} \right|$$

---

## **Matrix Operations for General Linear Regression**

Matrix operations play a crutial role in the mathematical representation and computation of linear regression. The general linear regession model can be represented as a matrix equation, which allows for efficient computation and manipulation of the model parameters.

Let's consider a linear regression model with m observation and n predictors, where the response variable y is related to the predicator variables $X_1, X_2, ..., X_n$ through the equation:

$$y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon$$

Here, $\beta_0, \beta_1, ..., \beta_n$ are the regression coefficients and $\epsilon$ is the error term.

To simplify the computation, we can represent the predictor variables in a design matrix X, where each row represents an observation and each column represents a predictor. The response varaible y can be represented as a vector. The regression coefficient can be represented as a vector  $\beta$.

$$X=
\begin{bmatrix}
1 & X_{11} & X_{12} & ... & X_{1n} \\
1 & X_{21} & X_{22} & ... & X_{2n} \\
... & ... & ... & ... & ... \\
1 & X_{m1} & X_{m2} & ... & X_{mn} \\
\end{bmatrix}$$

$$y = \begin{bmatrix}
y_1 \\
y_2 \\
... \\
y_m
\end{bmatrix}$$

$$\beta = \begin{bmatrix}\beta_0 \\\beta_1 \\... \\\beta_n\end{bmatrix}$$

The goal of linear regression is to find the optimal values of $\beta$ that minimize the sum of squared errors between the observed values of $y$ and the predicted values of $y$. This can be represented as the following optimization problem:

$$\beta^* = \arg\min_{\beta} \left\| y - X\beta \right\|^2$$

This optimization problem can be solved using various methods, such as the least squares method or gradient descent. The solution of this problem provides the optimal values of $\beta$ that best fit the data

---

## **Matrix Least Squares**

Matrix Least Squares is a method used to find the optimal values of the coefficients in a linear regression model. The method involves using matrices to perform linear algebra operations to minimize the sum of squares of the residuals (SSE), which measures the discrepancy between the observed and predicted values. The SSE is a measure of the goodness of fit of the model

The matric least squares method is based on finding the values of the regression coefficients (β0 and β1, ..., βp) that minimize the sum of squared errors between the observed response variablt (Y) and the predicted response variable (Ŷ) given the predictors (X).

The matrix formulation of the linear regression problem is given by the equation:

$$Y = X\beta + \epsilon$$

where $Y$ is a $n$ x $1$ vector of response values, $X$ is a $n$ x $p$ matrix of predictors, $\mathbf{\beta}$ is a $p$ x $1$ vector of regression coefficients, and $\mathbf{\epsilon}$ is a $n$ x $1$ vector of errors.

The total sum of squares (SSTO) measures the total variance in the response variable. It is calculated by summing the squares of the differences between each observed value and the mean of the response variable

$$SSTO = \sum_{i=1}^n (y_i - \bar{y})^2$$

The regression sum of squares (SSR) measures the explained variance in the response variable by the linear regerssion model. It is calculated by summing the squares of the differences between the predicted values and the mean of the response variable

$$SSR = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2$$

The ANOVA (analysis of variance) table provides a summary of the explained and residual variance in the resopnse variable by the linear regression model. The table consists of the following components:

* The regression sum of squares (SSR)

* The residual sum of squares (SSE)

* The total sum of squares (SSTO)

The ANOVA table can be constructed using these values, which provides a summary of the variation in the response variable explained by the predicators, the error, and the total variation in the resopnse variable.

---

## **Hypothesis Testing**

The ANOVA table can be used to perform hypothesis tests and make inferences about the significance of the independent variables in explaining the variation in the respose variable. The F-test in linear regression is used to test the null hypothesis that all the coefficients in the model are equal to zero. This test is used to determine thwther the model is significant, i.e., whether there is a relationship between the independent variables and the dependent variable. The F-test statisticis calculated as the ratio of the mean squared regression to the mean squared error:

$$F = \frac{MSR}{MSE}$$

Where MSR is the mean squared regeression and MSE is the means squared error. The degrees of freedom for the numerator is equal tot the number of independent variables on the model and the degrees of freedom for the denominator is equal tot the total number of observations minus the number of independent variables in the model.

The coefficient of multiple determination (R2) measures the propotion of the variance in the dependent variable that is explained bt the independent variables in the model. It is calculated as the ratio of the explained variance (SSR) and the total variance (SSTO):

$$R^2 = \frac{SSR}{SSTO}$$

The adjusted R2 takes into account the number of independent variables in the model and adjust the R2 to penalize models with a large number of independent variables relative to the number of observations

$$R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}$$

Where n is the total number of observations and p is the number of independent variables in the model

T-test are used to test the hypothesis that the coefficients of the independent variables in the model are equal to zero. The t-statistic is calcualated as the ration of the estimate of the coefficient to its standard error:

$$t = \frac{\hat{\beta_j}}{SE(\hat{\beta_j})}$$

Where $\hat{\beta_j}$ is the estimate of the coefficient and $SE(\hat{\beta_j})$ is its standard error. The null hypothesis is that the coefficient is equal to zero, and the alternative hypothesis is that the coefficient is not equal to zero. The p-value is then calculated using the t-distribution with n-p-1 degrees of freedom. If the p-value is less than a predetermined significance level, usually 0.05, the null hypothesis is rejected and the coefficient is considered to be statistically significant.

```python
# Extract predicator variables (remove categorical variables like team)

X = df[['DistanceCovered(InKms)',
        'Goals',
        'ShotsPerGame',
        'AgentCharges',
        'BMI',
        'Cost',
        'PreviousClubCost']]

y = df['Score']

```

---

## **Model Selection Criteria**
The Akaike information criterion (AIC) and the Bayesian information criterion (BIC) provide measures of midel perfomance that account for model complexity. AIC and BIC combine a term reflecting how well the model fites the data with a term that penalizes the model in proportion to its number of paramenters

---

## **Train - Test Split**

The data points are divided into two datasets, train and test, in a train test split method. The train data is used to train the model, and the model is theen used to predict on the test data to see how the model performs on unseen data and whether it is overfitting or underfitting.

As we discussed in the notebook for the previous part as we increase parameters or variables in the model, it becomes complex.

Complex models can result in overfitting and simple models can result in underfitting. Let's understand what is the sweet spot that we talked about.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/circles.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

### **Bias - Variance TradeOff**

**Bias**
The bias is defined as the difference between the ML model's prediction of the values and the correct value. Biasing causes a substantial inaccuracy in both training and testing data. To prevent the problem of underfitting, it is advised that an algorithm be low biassed at all times.
The data predicted by high bias is in a straight line format, and hence does not fit correctly in the dataset. This type of fitting is known as data underfitting. This occurs when the assumption is overly simple or linear.

**Variance**
The variance of the model is the variability of model prediction for a praticular data point, which tells us about the dispersion of our data. The model with large variance has a very complicated fit to the training data and so is unable to fit correctly on new data. As a result, while such models perform exceptionally well on training data, they have substantial error rates on test data.

The model basically tries to memorize the patterns in the training data and performs poorly when unseen data is presented. When a model has a large variance, this is referred to as Overfitting of Data.

We have to find a sweet spot, a model complexity when returns the minimum or optinal error.This tradeoff is known as Bias - Variance TradeOff.

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/complexity.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
# Splitting with 75% training, 25% testing
x_train, x_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    train_size = 0.75,
                                                    test_size = 0.25,
                                                    random_state = 100)

```

---

```python
# Fit the linear regression model
x_train_with_intercept = sm.add_constant(x_train)
lr = sm.OLS(y_train, x_train_with_intercept).fit()
print(lr.summary())

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/ans.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

We got an $R^2$ of 0.959 which is pretty great and the difference between $R^2$ and adjusted $R^2$ is not very significant which is also great sign. Let's try to eliminate some variables and see if we can improve our results.

---

```python
# Can we trim some variables and see how it performs?
X = df[['DistanceCovered(InKms)',
        'BMI',
        'Cost',
        'PreviousClubCost']]

X_train, X_test ,y_train, y_test = train_test_split(X,
                                                y,
                                                train_size = 0.75,
                                                test_size = 0.25,
                                                random_state = 100)

X_train_with_intercept = sm.add_constant(x_train)
lr = sm.OLS(y_train,
        x_train_with_intercept).fit()

print(lr.summary())

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/ans3.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

The AIC and VIC both dropped even through we could't see much change in $R^2$. This suggest this model is better than the previous one. Let's see if "Club" information can throw anything els for us. But before that, lets understand the types of data, why is it important to handle categorical data and how we do so.

---

## **Types of Data**

**Numerical Data** - Data that is expressed as number  rather than in any linguistic or descreptive form is rellered to as numerical data or quantitative data. These are basically of two types:

* **Discrete** - Discrete data is a sort of numerical data that consists of full, tangible numbers that have particular and definite data values established by counting. For example, number of ballons.
* **Continuous** - Continuous data consists of floating numbers and fluctuating data values taken over a specific time period. For example, temperature

**Categorical Data** - The data under consideration is described using a finite number of discrete classification in qualitative or categorical data. In other words, this kind of data cannot simply be measured or tallied using numbers and must instead be categorised. A excellent illustration of this data type is the gender of a person - male, female or other.

Subcategories under this include the following two:
- Nominal These are the set of values that don't possess a natural ordering. These are simply naming variables. For example, color variable can be blue, green, black and it does not contain a natural order.
-  These types of values have a natural ordering while maintaining their class of values. If we consider the size of a clothing brand then we can easilt sort them accourding to their name tag in the order of small < medium < large. The reviewing system in restaurants or hotels can also be sconsidered as an ordinal data type where Excellent service is definetely better that good service.

These categories assist us in determining which encoding technique should be used for a given type of data. Because machine learning models are mathematical in nature, it is necessary to translate qualitative input into numerical kinds because they cannot handle these values directly.

Given the smaller number, one-hot encoding, which is comparable to binary coding, can be used for nominal data types where there is no comparison between the categories. For ordinal data types, label encoding, a type of integer encoding, can be used.

---

## **One - Hot Encoding**

Most Machine LEarning algorithms are unable to deal with categorical data and must be transformed to numberical data. What are our options for converting categorical data to numerical data? Should we number the labels? For example, red represents 1. It will cause a bias in the model since the model will consider blue to be superior to red.
To address this, we use a categorical data encoding approach known as one hot encoding. In this method, we nuild a new hfeature for each label and assign it a value of 1. For example, if blue is present, it marked as 1, else it is 0

### **Dummy Variables**

Pandas get_dummies() converts categorical data into indicato variables.

**Data**: array-like, Series or DataFrame, Data of which to get dummy indicators.

**Prefix**: str, list of str, or dict or str, default None, String to append DAtaFrame column names.

**drop_first**: bool, default False, Whether to get k-1 dummies out of k categorical levels by removing the first level.
Refer to the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) for more information.

---

## **Dummy Variable Trap in Regression Models**

When two or more dummy variables formed using one-hot encoding are significantly connected, the Dummy Variable Trap arises (multi-collinear).

This indicates that one variable may be inferred from the others, making predicted coefficient variables in regression models difficult to understand.

When you have significant categorical features, one hot encoding them can result in multicollinearity. In other words, due to multicollinearity, the individual influence of the dummy variables on the prediction model cannot be effectively interpreted.

---

```python
print(df.dtypes)

```

---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/SA.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---

```python
# Let's try throwing in club data using dummy variables
clubs=set(df.Club)
clubs

nominal_features = pd.get_dummies(df['Club'])
nominal_features.head()

```

---

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CHE</th>
      <th>LIV</th>
      <th>MUN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>

---

## **Data Concatenation**
The pandas concat() function is used to concatenate pandas objects along a particular axis with optionsal set of logic along the other axes.

**objs**: a sequence or mapping of Series or DataFrame objects

**axis**: {0/’index’, 1/’columns’}, default 0

Here we are concatenating nominal features with our dataframe with axis = 1, which means horizontally. Refer to the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) for more information.

---

```python
df = pd.concat([df, nominal_features], axis = 1)
df.head()

```

---

<div style="display: flex; justify-content: center; width: 100%; overflow-x: auto;">
<div style="max-width: 100%;">
<style scoped>
    .dataframe {
        margin: 0 auto;  /* Centers the table */
        width: auto;     /* Allows table to size to content */
        max-width: 100%; /* Prevents table from overflowing container */
    }
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: center;
        white-space: nowrap; /* Prevents header text from wrapping */
    }
    .dataframe tbody td {
        white-space: nowrap; /* Prevents cell text from wrapping */
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>PlayerName</th>
      <th>Club</th>
      <th>DistanceCovered(InKms)</th>
      <th>Goals</th>
      <th>MinutestoGoalRatio</th>
      <th>ShotsPerGame</th>
      <th>AgentCharges</th>
      <th>BMI</th>
      <th>Cost</th>
      <th>PreviousClubCost</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Score</th>
      <th>CHE</th>
      <th>LIV</th>
      <th>MUN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Braund, Mr. Owen Harris</td>
      <td>MUN</td>
      <td>3.96</td>
      <td>7.5</td>
      <td>37.5</td>
      <td>12.3</td>
      <td>60</td>
      <td>20.56</td>
      <td>109.1</td>
      <td>63.32</td>
      <td>195.9</td>
      <td>78.9</td>
      <td>19.75</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Allen, Mr. William Henry</td>
      <td>MUN</td>
      <td>4.41</td>
      <td>8.3</td>
      <td>38.2</td>
      <td>12.7</td>
      <td>68</td>
      <td>20.67</td>
      <td>102.8</td>
      <td>58.55</td>
      <td>189.7</td>
      <td>74.4</td>
      <td>21.30</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Moran, Mr. James</td>
      <td>MUN</td>
      <td>4.14</td>
      <td>5.0</td>
      <td>36.4</td>
      <td>11.6</td>
      <td>21</td>
      <td>21.86</td>
      <td>104.6</td>
      <td>55.36</td>
      <td>177.8</td>
      <td>69.1</td>
      <td>19.88</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>McCarthy, Mr. Timothy J</td>
      <td>MUN</td>
      <td>4.11</td>
      <td>5.3</td>
      <td>37.3</td>
      <td>12.6</td>
      <td>69</td>
      <td>21.88</td>
      <td>126.4</td>
      <td>57.18</td>
      <td>185.0</td>
      <td>74.9</td>
      <td>23.66</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Palsson, Master. Gosta Leonard</td>
      <td>MUN</td>
      <td>4.45</td>
      <td>6.8</td>
      <td>41.5</td>
      <td>14.0</td>
      <td>29</td>
      <td>18.96</td>
      <td>80.3</td>
      <td>53.20</td>
      <td>184.6</td>
      <td>64.6</td>
      <td>17.64</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>
</div>

---

```python
print(df.dtypes)

```

---

PlayerName                 object
Club                       object
DistanceCovered(InKms)    float64
Goals                     float64
MinutestoGoalRatio        float64
ShotsPerGame              float64
AgentCharges                int64
BMI                       float64
Cost                      float64
PreviousClubCost          float64
Height                    float64
Weight                    float64
Score                     float64
dtype: object

---

```python
df_numeric = df.select_dtypes(include=[np.number])

```

---

# Run with club encoding

```python
X = df_encoded[['DistanceCovered(InKms)',
                'BMI',
                'Cost',
                'PreviousClubCost',
                'CHE',
                'MUN',
                'LIV']]

x_train, x_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    train_size = 0.75,
                                                    test_size = 0.25,
                                                    random_state = 100)

x_train_with_intercept = sm.add_constant(x_train)

```

---

# Run with club encoding

```python
lr = sm.OLS(y_train, x_train_with_intercept).fit()

print(lr.summary())

```

---

```python
# Look at model plot
plt.figure(figsize=(8, 6), dpi=80)
x_test_with_intercept = sm.add_constant(x_test)
y_test_fitted = lr.predict(x_test_with_intercept)

# plot the scatter plot between the fitted values and actual test values for repsonse variable
plt.scatter(y_test_fitted, y_test)
plt.xlabel("Y Predicted")
plt.ylabel("Y Actual")
plt.title("Scatter plot between Fitted and Actual Test Values")
plt.show()

```

---


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/scatter.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This image can also have a caption. It's like magic.
</div>

---
